ğŸ‘ Pytorchä½¿ç”¨è¯¦è§£

---
[TOC]

---
## å®‰è£…
å‚è€ƒå¹¶ä»”ç»†é˜…è¯»[å®˜ç½‘pytorchå®‰è£…æ•™ç¨‹](https://pytorch.org/get-started/locally/)

## Warning
**ï¼ˆ1ï¼‰UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.**
**è§£å†³æ–¹æ¡ˆï¼š** é—®é¢˜å‡ºç°åœ¨class LabelSmoothingï¼Œå°† "self.criterion = nn.KLDivLoss(size_average=False)" æ”¹ä¸º "self.criterion = nn.KLDivLoss(reduction='sum')"

## Error

## åŸºæœ¬å‡½æ•°
### torch.tensor()
ç®€ä»‹ï¼šæ ¹æ®å·²æœ‰çš„æ•°æ®ç”Ÿæˆtorchç±»å‹çš„tensorã€‚
è¯­æ³•ï¼š
```python
torch.tensor(data, dtype=None, device=None, requires_grad=False)
```
ç¤ºä¾‹ï¼š
ä¾‹1ï¼š
```python
>>> import torch
>>> ss = [1,2]
>>> torch.tensor(ss)
tensor([1, 2])
>>> torch.tensor(ss, dtype=torch.float)
tensor([1., 2.])
```

### tensor_obj.flatten()
ç®€ä»‹ï¼šé™ä½tensorçš„ç»´åº¦ã€‚

**å‚è€ƒèµ„æ–™ï¼š**
1. https://zhuanlan.zhihu.com/p/587064877

### tensor_obj.unsqueeze()
ç®€ä»‹ï¼šå‡é«˜tensorçš„ç»´åº¦ã€‚

**å‚è€ƒèµ„æ–™ï¼š**
1. https://blog.csdn.net/weixin_53142585/article/details/128997156

### torch.eq()
ç®€ä»‹ï¼šå¯¹ä¸¤ä¸ªTensorè¿›è¡Œé€å…ƒç´ çš„æ¯”è¾ƒï¼Œè‹¥ç›¸åŒä½ç½®çš„ä¸¤ä¸ªå…ƒç´ ç›¸åŒï¼Œåˆ™è¿”å›Trueï¼›è‹¥ä¸åŒï¼Œè¿”å›Falseã€‚
```python
>>> import torch
>>> a = torch.tensor([1,2,3,4,5,6,7])
>>> b = torch.tensor([1,2,3,5,6,7,8])
>>> torch.eq(a, b)
tensor([ True,  True,  True, False, False, False, False])
>>> torch.sum(torch.eq(a, b))
tensor(3)
```

### torch.clamp()
ç®€ä»‹ï¼šå°†è¾“å…¥inputå¼ é‡æ¯ä¸ªå…ƒç´ çš„å¤¹ç´§åˆ°åŒºé—´ [min,max]ï¼Œå¹¶è¿”å›ç»“æœåˆ°ä¸€ä¸ªæ–°å¼ é‡ã€‚æ“ä½œé€»è¾‘å¦‚ä¸‹æ‰€ç¤ºï¼š
```txt
      | min, if x_i < min
y_i = | x_i, if min <= x_i <= max
      | max, if x_i > max
```
```python
>>> import torch
>>> a = torch.tensor([1,-2,-3,4,5,-6,-7])
>>> torch.clamp(a, 0, 1)
tensor([1, 0, 0, 1, 1, 0, 0])
```

### tensor.shape
ç®€ä»‹ï¼šæŸ¥çœ‹tensorçš„æ•°æ®ç»´åº¦ã€‚
```python
>>> import torch
>>> a = torch.randn(8,4,9)
>>> a.shape
torch.Size([8, 4, 9])
```

### tensor.item()
ç®€ä»‹ï¼šç”¨äºåœ¨åªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„tensorä¸­æå–å€¼ï¼Œæ³¨æ„æ˜¯åªåŒ…å«ä¸€ä¸ªå…ƒç´ ï¼Œå¦åˆ™çš„è¯ä½¿ç”¨.tolist()ã€‚
```python
>>> import torch
>>> x = torch.tensor([1])
>>> x
tensor([1])
>>> print(x.item())
1
>>> y = torch.tensor([2,3,4,5])
>>> print(y.item())
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ValueError: only one element tensors can be converted to Python scalars
```

### tensor.tolist()
ç®€ä»‹ï¼šå¼ é‡è½¬åŒ–ä¸ºåˆ—è¡¨ã€‚æ­¤æ—¶è½¬åŒ–çš„åˆ—è¡¨æ˜¯ç”±éå¼ é‡çš„æ•°å€¼ç»„æˆçš„åˆ—è¡¨ã€‚
```python
>>> import torch
>>> t1 = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
>>> print(t1.tolist())
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
```

### list(tensor)
ç®€ä»‹ï¼šå¼ é‡è½¬åŒ–ä¸ºåˆ—è¡¨ã€‚æ­¤æ—¶è½¬åŒ–çš„åˆ—è¡¨æ˜¯ç”±ä¸€ä¸ªä¸ªé›¶ç»´å¼ é‡æ„æˆçš„åˆ—è¡¨ï¼Œè€Œéå¼ é‡çš„æ•°å€¼ç»„æˆçš„åˆ—è¡¨ã€‚
```python
>>> import torch
>>> t1 = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
>>> list(t1)
[tensor(1), tensor(2), tensor(3), tensor(4), tensor(5), tensor(6), tensor(7), tensor(8), tensor(9), tensor(10)]
```

### tensoræ‹¼æ¥-torch.cat()
```python
>>> import torch
>>> input = torch.randn(2,5)
>>> input
tensor([[ 0.1736,  0.0204, -0.6572,  0.2260,  0.8728],
        [-0.7299, -1.1448, -0.1648, -1.6386,  0.6957]])
>>> input.unsqueeze_(1)
tensor([[[ 0.1736,  0.0204, -0.6572,  0.2260,  0.8728]],

        [[-0.7299, -1.1448, -0.1648, -1.6386,  0.6957]]])
>>> input
tensor([[[ 0.1736,  0.0204, -0.6572,  0.2260,  0.8728]],

        [[-0.7299, -1.1448, -0.1648, -1.6386,  0.6957]]])
>>> lt=[]
>>> for t in input:
...     lt.append(t)
...
>>> lt
[tensor([[ 0.1736,  0.0204, -0.6572,  0.2260,  0.8728]]), tensor([[-0.7299, -1.1448, -0.1648, -1.6386,  0.6957]])]
>>> torch.cat(lt, dim=0)
tensor([[ 0.1736,  0.0204, -0.6572,  0.2260,  0.8728],
        [-0.7299, -1.1448, -0.1648, -1.6386,  0.6957]])
```

## åŸºæœ¬è¿ç®—å‡½æ•°
### torch.exp()
ç®€ä»‹ï¼šeä¸ºåº•çš„æŒ‡æ•°å‡½æ•°ã€‚
```python
>>> import torch
>>> torch.exp(0)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: exp(): argument 'input' (position 1) must be Tensor, not int
>>> torch.exp(torch.tensor([0,]))
tensor([1.])
>>> torch.exp(torch.tensor([1,]))
tensor([2.7183])
>>> torch.exp(torch.tensor([2,]))
tensor([7.3891])
```

### tensor.exp()
ç®€ä»‹ï¼šeä¸ºåº•çš„æŒ‡æ•°å‡½æ•°ã€‚
```python
>>> import torch
>>> x = torch.FloatTensor([1, 2, 3])
>>> x.exp()
tensor([ 2.7183,  7.3891, 20.0855])
```

### torch.randn_like()
ç®€ä»‹ï¼šè¿”å›ä¸€ä¸ªå’Œè¾“å…¥å¤§å°ç›¸åŒçš„å¼ é‡ï¼Œç”±å‡å€¼ä¸º0ã€æ–¹å·®ä¸º1çš„æ ‡å‡†æ­£æ€åˆ†å¸ƒå¡«å……ã€‚

### torch.randn()
ç®€ä»‹ï¼šç”Ÿæˆéšæœºæ•°å­—çš„tensorï¼Œè¿™äº›éšæœºæ•°å­—æ»¡è¶³æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼ˆ0~1ï¼‰ã€‚
```python
>>> import torch
>>> torch.randn(1, 1, 50, device="cuda:0")
tensor([[[-0.6656, -0.3174, -0.5807,  0.8547, -0.0651, -0.7386, -1.0479,
           0.5800,  1.2591, -0.8286,  0.7106,  0.0404, -0.0539, -0.5954,
           1.2302, -1.0276, -1.2646, -0.9157, -0.2282,  2.5122, -0.1437,
           0.9583,  0.1726, -0.1790,  0.6030,  0.4744,  0.4201,  1.9546,
           0.9679, -0.1128, -0.6830, -1.0865,  0.0938, -0.6147, -0.5624,
          -0.7046,  0.7411, -0.9382,  1.6949,  1.1044,  1.3029,  1.1905,
           0.9863, -0.6040, -0.8097,  0.6685, -2.1447, -0.6545,  0.0234,
          -1.1524]]], device='cuda:0')
>>> torch.randn(1, 1, 1, 50, device="cuda:0")
tensor([[[[ 1.1982,  0.9101, -1.1776,  1.5119,  0.0992,  0.2000, -0.3951,
           -0.3701, -0.4243, -0.4809, -0.1055, -1.5904, -0.4032,  0.7403,
           -0.3815,  1.1766,  1.2101,  0.3082,  1.0410,  1.2124, -0.6343,
            0.3002, -1.3906, -1.9843,  0.4277,  1.3072,  0.1649,  0.9681,
            0.7385,  0.0229, -0.0233,  0.3757,  1.6127, -0.6815,  1.2074,
            1.7308, -0.5322, -0.7606, -1.0208, -0.1202, -0.2802,  0.7776,
           -0.1678, -2.8138,  0.0833,  0.5150,  1.3218,  1.6249, -0.0213,
            1.3156]]]], device='cuda:0')
>>> torch.randn(1, 1, 1, 1, 50, device="cuda:0")
tensor([[[[[-1.7176,  2.0326, -0.3583, -1.2484,  0.0724, -0.4752,  1.5606,
            -0.0277,  1.3331,  1.0722, -1.2756,  0.0299,  0.7153, -1.8144,
             1.7690,  0.6880,  0.3205, -0.0977,  0.4175, -1.2281, -0.3711,
            -0.5029, -0.5313,  0.9933,  0.1168, -1.3517,  0.3544, -0.4375,
             0.9703,  0.9564,  0.8907,  1.1416, -0.9067,  0.0682, -0.5309,
             0.1293,  0.8388,  0.4383, -0.7352,  0.5400, -1.7906,  1.0049,
             1.4189,  0.5314,  1.0470,  0.1717,  1.6306,  1.0404, -0.6482,
             1.0678]]]]], device='cuda:0')
>>> torch.randn(1, 50, device="cuda:0")
tensor([[ 1.0000,  0.8019, -1.3283, -1.5774, -0.7373,  0.8933,  0.6255,  0.2343,
          1.3824,  2.9639, -0.7921, -0.7666,  0.3089,  0.0469,  0.5776, -0.3141,
          0.6909,  0.4588, -1.0630,  0.0577,  1.2595, -1.5326, -0.2966,  0.7798,
          1.7747, -0.6188, -0.1917, -0.2683,  1.0564, -1.2342,  1.0294,  0.0549,
          0.6450,  0.0092,  0.1396, -0.5071,  1.4961, -2.0883,  0.5575, -1.9708,
          1.2327, -1.3869,  0.2890, -0.4493,  0.3397, -0.7289, -1.1132,  0.7367,
          2.3925, -0.8641]], device='cuda:0')
>>> torch.randn(50, device="cuda:0")
tensor([ 0.6443, -0.8284, -1.1112, -0.4010,  1.0799, -0.1044, -0.5971, -1.1391,
         0.2200,  0.5723,  1.1666,  2.0415,  1.7110, -0.5594,  0.5746,  0.1688,
         0.0071,  0.1042,  0.6987,  0.7494, -0.5613, -0.4290, -0.9476, -0.3628,
        -0.0442,  0.5232,  0.0771, -1.0056,  0.0905,  1.0298, -1.5645, -0.9261,
        -1.5188,  0.1830, -0.8740,  0.1861,  1.6453,  0.8969,  1.5759, -0.6142,
         0.6832, -1.3723, -0.2708, -1.8894, -1.6328, -0.8100, -0.3694,  0.1364,
         1.2177, -0.8820], device='cuda:0')
```

### torch.abs()
ç®€ä»‹ï¼šå°†å‚æ•°ä¼ é€’åˆ° torch.abs åè¿”å›è¾“å…¥å‚æ•°çš„ç»å¯¹å€¼ä½œä¸ºè¾“å‡ºï¼Œè¾“å…¥å‚æ•°å¿…é¡»æ˜¯ä¸€ä¸ª Tensor æ•°æ®ç±»å‹çš„å˜é‡ã€‚
```python
>>> import torch
>>> a = torch.tensor([1, -2, -3, 4, 5, -6, -7])
>>> torch.abs(a)
tensor([1, 2, 3, 4, 5, 6, 7])
```

## æ¨¡å‹å‚æ•°ç›¸å…³å‡½æ•°æ–¹æ³•
### model.parameters()
ç®€ä»‹ï¼šmodel.parameters()ä¿å­˜çš„æ˜¯Weightså’ŒBaiså‚æ•°çš„å€¼ã€‚
![](Pytorchä½¿ç”¨è¯¦è§£/Pytorchä½¿ç”¨è¯¦è§£_2023-03-04-16-37-45.png)

**å‚è€ƒèµ„æ–™ï¼š**
1. https://blog.csdn.net/weixin_42843425/article/details/122518502


## ä¼˜åŒ–ç®—æ³•
### torch.optim.Adam()
ç®€ä»‹ï¼š
è¯­æ³•ï¼š
```python
torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
# params (iterable) â€“ å¾…ä¼˜åŒ–å‚æ•°çš„iterableæˆ–è€…æ˜¯å®šä¹‰äº†å‚æ•°ç»„çš„dict
# lr (float, å¯é€‰) â€“ å­¦ä¹ ç‡ï¼ˆé»˜è®¤ï¼š1e-3ï¼‰
# betas (Tuple[float, float], å¯é€‰) â€“ ç”¨äºè®¡ç®—æ¢¯åº¦ä»¥åŠæ¢¯åº¦å¹³æ–¹çš„è¿è¡Œå¹³å‡å€¼çš„ç³»æ•°ï¼ˆé»˜è®¤ï¼š0.9ï¼Œ0.999ï¼‰
# eps (float, å¯é€‰) â€“ ä¸ºäº†å¢åŠ æ•°å€¼è®¡ç®—çš„ç¨³å®šæ€§è€ŒåŠ åˆ°åˆ†æ¯é‡Œçš„é¡¹ï¼ˆé»˜è®¤ï¼š1e-8ï¼‰
# weight_decay (float, å¯é€‰) â€“ æƒé‡è¡°å‡ï¼ˆL2æƒ©ç½šï¼‰ï¼ˆé»˜è®¤: 0ï¼‰
```

## æ¢¯åº¦ä¸åå‘ä¼ æ’­
### loss.backward()
**ä¸€èˆ¬ç”¨æ³•ï¼š**
```python
optimizer.zero_grad()   # æ¸…ç©ºè¿‡å¾€çš„æ¢¯åº¦
loss.backward()   # åå‘ä¼ æ’­ï¼Œè®¡ç®—å½“å‰çš„æ¢¯åº¦
optimizer.step()   # æ ¹æ®æ¢¯åº¦æ›´æ–°ç½‘ç»œå‚æ•°
```
**Loss.backward(retain_graph=True)çš„æ„ä¹‰å°±æ˜¯ä¿ç•™ä¸­é—´å‚æ•°ï¼Œä¸æ¸…ç©ºä¸­é—´å˜é‡ã€‚Pytorchä¸­æœ‰å¤šæ¬¡backwardæ—¶éœ€è¦retain_graphå‚æ•°ï¼ŒPytorchä¸­çš„æœºåˆ¶æ˜¯æ¯æ¬¡è°ƒç”¨loss.backward()æ—¶éƒ½ä¼šfreeæ‰è®¡ç®—å›¾ä¸­æ‰€æœ‰ç¼“å­˜çš„buffersï¼Œå½“æ¨¡å‹ä¸­å¯èƒ½æœ‰å¤šæ¬¡backward()æ—¶ï¼Œæ­£ç¡®çš„åšæ³•åº”è¯¥æ˜¯å‰é¢çš„backwardéƒ½åº”è¯¥ä¿ç•™ä¸­é—´å‚æ•°ä½¿ç”¨retain_graphï¼Œæœ€åä¸€æ¬¡è¦æ¸…ç©ºç¼“å­˜ï¼Œä¸ä½¿ç”¨retain_graphã€‚**
```python
optimizer.zero_grad()   # æ¸…ç©ºè¿‡å¾€æ¢¯åº¦ï¼›
loss1.backward(retain_graph=True)   # åå‘ä¼ æ’­ï¼Œè®¡ç®—å½“å‰æ¢¯åº¦ï¼›
loss2.backward()   # åå‘ä¼ æ’­ï¼Œè®¡ç®—å½“å‰æ¢¯åº¦ï¼›
optimizer.step()   # æ ¹æ®æ¢¯åº¦æ›´æ–°ç½‘ç»œå‚æ•°
```

### with torch.set_grad_enabled(False):ä¸with torch.no_grad():
ä¸¤ä¸ªæ˜¯ç­‰ä»·çš„ï¼Œä½œç”¨æ˜¯å–æ¶ˆå¯¹è®¡ç®—æ“ä½œçš„è·Ÿè¸ªã€‚withå—ä¸‹å®šä¹‰çš„æ‰€æœ‰èŠ‚ç‚¹çš„requires_gradéƒ½ä¸ºFalseï¼Œgrad_fnéƒ½ä¸ºNoneï¼ˆç›¸å½“äºwithå—ä¸­å®šä¹‰çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½è¢«detachäº†ï¼‰ã€‚æ³¨æ„è¯¥è¯­å¥ä¸æ”¹å˜withå—å¤–å®šä¹‰çš„èŠ‚ç‚¹çš„requires_gradå’Œgrad_fnã€‚

### model.eval()


### with torch.set_grad_enabled(False):/with torch.no_grad()å’Œmodel.eval()çš„åŒºåˆ«ä¸è”ç³»
**model.eval()ä¼šå½±å“å„å±‚çš„gradientè®¡ç®—è¡Œä¸ºï¼Œå³gradientè®¡ç®—å’Œå­˜å‚¨ä¸trainingæ¨¡å¼ä¸€æ ·ï¼Œåªæ˜¯ä¸è¿›è¡Œåä¼ ã€‚with torch.zero_grad()åˆ™åœæ­¢autogradæ¨¡å—çš„å·¥ä½œï¼Œä¹Ÿå°±æ˜¯åœæ­¢gradientè®¡ç®—ï¼Œä»¥èµ·åˆ°åŠ é€Ÿå’ŒèŠ‚çœæ˜¾å­˜çš„ä½œç”¨ï¼Œ** ä»è€ŒèŠ‚çœäº†GPUç®—åŠ›å’Œæ˜¾å­˜ï¼Œä½†æ˜¯å¹¶ä¸ä¼šå½±å“dropoutå’Œbatchnormå±‚çš„è¡Œä¸ºã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœä¸åœ¨æ„æ˜¾å­˜å¤§å°å’Œè®¡ç®—æ—¶é—´çš„è¯ï¼Œä»…ä½¿ç”¨model.eval()å·²è¶³å¤Ÿå¾—åˆ°æ­£ç¡®çš„validationçš„ç»“æœï¼›è€Œwith torch.zero_grad()åˆ™æ˜¯æ›´è¿›ä¸€æ­¥åŠ é€Ÿå’ŒèŠ‚çœgpuç©ºé—´ï¼ˆå› ä¸ºä¸ç”¨è®¡ç®—å’Œå­˜å‚¨gradientï¼‰ï¼Œä»è€Œå¯ä»¥æ›´å¿«è®¡ç®—ï¼Œä¹Ÿå¯ä»¥è·‘æ›´å¤§çš„batchæ¥æµ‹è¯•ã€‚
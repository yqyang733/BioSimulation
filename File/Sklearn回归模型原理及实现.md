👏 Sklearn|回归模型原理及实现

---
[TOC]

---
## 梯度提升回归（树）模型
### 简介
梯度提升回归（树）模型：Gradient boosting regression（GBR），是一种决策树集成方法。通过合并多个决策树来构建一个更为强大的模型。可用于分类也可用于回归。

### 原理
与随机森林方法不同，梯度提升采用连续的方式构造树，每棵树都试图纠正前一棵树的错误。主要思想是合并许多简单的模型，比如深度较小的树。每棵树只能对部分数据做出好的预测，因此，添加的树越来越多，可以不断迭代提高性能。

### 注意事项
（1）梯度提升决策树是监督学习中最强大也最常用的模型之一。
（2）梯度提升树通常对参数设置更为敏感，正确的设置参数，可以将精度提高很多。需要仔细调参，训练时间较长，通常不适用于高维稀疏数据。
（3）梯度提升的一个重要参数是learning_rate（学习率），它用于控制每棵树纠正前一棵树的错误的强度。较高的学习率意味着每棵树都可以做出较强的修正，这样模型更为复杂。另一个重要参数是n_estimators，通过增大n_estimators向集成中添加更多树，也可以增加模型复杂度，因为模型有更多机会纠正训练集上的错误。这两个参数高度相关，因为 learning_rate越低，就需要更多的树来构建具有相似复杂度的模型。随机森林的n_estimators值总是越大越好，但梯度提升不同，增大n_estimators会导致模型更加复杂，进而可能导致过拟合。通常的做法是根据时间和内存的预算选择合适的n_estimators，然后对不同的learning_rate进行遍历。还有一个重要参数是max_depth（或max_leaf_nodes），用于降低每棵树的复杂度。梯度提升模型的max_depth通常都设置得很小，一般不超过5。

### 参考资料
1. [决策树集成：梯度提升回归树](https://www.jianshu.com/p/17368988d6d9)
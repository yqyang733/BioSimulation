ğŸ‘ å·¥å…·|MultiDTI|å»ºç«‹åˆ†ç±»æ¨¡å‹é¢„æµ‹è¯ç‰©é¶æ ‡ç›¸äº’ä½œç”¨(å°æ•°æ®)

---

[å°é¢PPT](../PPT/å·¥å…·MultiDTIå»ºç«‹åˆ†ç±»æ¨¡å‹é¢„æµ‹è¯ç‰©é¶æ ‡ç›¸äº’ä½œç”¨(å°æ•°æ®).pptx)

---

[TOC]

---

## ç”Ÿæˆè¾“å…¥æ–‡ä»¶

### SMILESå¤„ç†

å‚è€ƒï¼š[å·¥å…·|DeepPurpose|MPNN-CNNå»ºç«‹åˆ†ç±»/å›å½’æ¨¡å‹é¢„æµ‹è¯ç‰©é¶æ ‡ç›¸äº’ä½œç”¨(å°æ•°æ®)](./å·¥å…·DeepPurposeMPNN-CNNå»ºç«‹åˆ†ç±»å›å½’æ¨¡å‹é¢„æµ‹è¯ç‰©é¶æ ‡ç›¸äº’ä½œç”¨å°æ•°æ®.md)ï¼Œåªéœ€è¿›è¡Œæ­£åˆ™åŒ–å’Œå»ç›å³å¯ã€‚

### ç”Ÿæˆ drug_smiles, protein_fasta, drug_protein æ–‡ä»¶

**æ³¨æ„**ï¼šé¡ºåºä¸€å®šè¦å¯¹ä¸Šã€‚

ï¼ˆ1ï¼‰ä¸‰ä¸ªæ–‡ä»¶æ ¼å¼å¦‚PPTä¸­å›¾ç‰‡æ‰€ç¤º

ï¼ˆ2ï¼‰è„šæœ¬å¦‚ä¸‹æ‰€ç¤ºï¼š

matrix.py

```python
import sys
import numpy as np

def pre_process(inpt_1):
    with open(inpt_1) as f:
        f1 = f.readlines()
    lst_total = []
    lst_icv = []
    lst_uniprot = []
    for line in f1:
        icv = line.strip().split(",")[0]
        uniprot = line.strip().split(",")[1]
        lst_total.append([icv,uniprot])
        lst_icv.append(icv)
        lst_uniprot.append(uniprot)
    return lst_total, lst_icv, lst_uniprot

def network(lst_total, lst_icv, lst_uniprot):
    lst_icv_unique = list(set(lst_icv))
    lst_uniprot_unique = list(set(lst_uniprot))
    matrix_1 = np.zeros((len(lst_icv_unique),len(lst_uniprot_unique)))
    for i in range(len(lst_icv_unique)):
        for j in range(len(lst_uniprot_unique)):
            if [lst_icv_unique[i],lst_uniprot_unique[j]] in lst_total:
                matrix_1[i][j] += 1
    output = ""
    output += "network"
    output_icv = ""
    output_pro = ""
    for a in lst_uniprot_unique:
        output += ","
        output += a
        output_pro = output_pro + a + "\n"
    output += "\n"
    for b in range(len(lst_icv_unique)):
        output_icv = output_icv + lst_icv_unique[b] + "\n"
        output += lst_icv_unique[b]
        for c in matrix_1[b]:
            output += ","
            output += str(c)
        output += "\n"
    with open("result_matrix.txt","w") as rt, open("result_icv.txt","w") as rt_icv, open("result_pro.txt","w") as rt_pro:
        rt.write(output)
        rt_icv.write(output_icv)
        rt_pro.write(output_pro)

def main():
    inpt_1 = str(sys.argv[1])
    lst_total, lst_icv, lst_uniprot = pre_process(inpt_1)
    network(lst_total, lst_icv, lst_uniprot)

if __name__=="__main__":
    main()
```

pick.py

```python
import sys

def pre_process(inpt_1, inpt_2):
    dic = {}
    lst = []
    with open(inpt_1) as f:
        f1 = f.readlines()
    for line in f1:
        lst.append(line.strip())
    with open(inpt_2) as f2:
        f3 = f2.readlines()
    for line in f3:
        icv = line.strip().split(",")[0]
        smiles = line.strip().split(",")[1]
        dic[icv] = smiles
    output = ""
    for a in lst:
        output = output + a + "," + dic[a] + "\n"
    with open("protein_fasta_outtest.txt", "w") as rt:
        rt.write(output)

def main():
    inpt_1 = str(sys.argv[1])
    inpt_2 = str(sys.argv[2])
    pre_process(inpt_1, inpt_2)

if __name__=="__main__":
    main()
```

### ç”Ÿæˆè¯å…¸

```python
#å¯¹åºåˆ—å‹ç¬¦å·è¿›è¡Œåˆ†è¯ï¼Œæˆ‘ä»¬é‡‡ç”¨æ˜¯n-gramçš„æ–¹æ³•ï¼Œå°†nä¸ªå­—ç¬¦ä½œä¸ºä¸€ä¸ªåˆ†è¯å•ä½ï¼Œç”¨é”™å¼€ä¸€ä¸ªå­—ç¬¦çš„æ–¹æ³•æ„é€ å‡ºnä¸ªå­åºåˆ—
#è€ŒæŸä¸ªåºåˆ—çš„é«˜ç»´å‘é‡è¡¨ç¤ºå°±æ˜¯è¿™ä¸ªå­åºåˆ—çš„å¯¹åº”å‘é‡çš„å¹³å‡å€¼ã€‚
#å¯¹äºfastaåºåˆ—ï¼Œ3ä¸ªæ°¨åŸºé…¸å¯èƒ½æœ‰ç‰¹æ®Šçš„å«ä¹‰ï¼Œæ‰€ä»¥ä¸€èˆ¬ä½¿ç”¨3-gram
#å¯¹äºsmilesåºåˆ—ï¼Œè¿™é‡Œå°†å•ä¸ªå­—ç¬¦ä½œä¸ºä¸€ä¸ªè¯
#åœ¨è¿™é‡Œæˆ‘ä»¬æ„é€ smilesåºåˆ—å’Œfastaåºåˆ—çš„è¯æ±‡è¡¨

import pandas as pd
import pickle

smile_gram = 1
fasta_gram = 3
smi_dict_set = set()
fas_dict_set = set()
one_fas_dict_set = set()

smi_max_len = 0
fas_max_len = 0

smile = pd.read_csv("drug_smiles.csv",header=None,index_col=None)
fasta = pd.read_csv("protein_fasta.csv",header=None,index_col=None)

for item in smile.values:
    if len(item[1]) < smile_gram:
        while len(item[1]) < smile_gram:
            item[1] = item[1] + "_"
        smi_dict_set.add(item[1])
    else:
        for i in range(len(item[1]) - smile_gram + 1):
            smi_dict_set.add(item[1][i:i+smile_gram])

    if len(item[1]) > smi_max_len:
        smi_max_len = len(item[1])

for item in fasta.values:
    if len(item[1]) < fasta_gram:
        while len(item[1]) < smile_gram:
            item[1] = item[1] + "_"
        fas_dict_set.add(item[1])
    else:
        for i in range(len(item[1]) - fasta_gram +1):
            fas_dict_set.add(item[1][i:i+fasta_gram])

    if len(item[1]) > fas_max_len:
        fas_max_len = len(item[1])

for item in fas_dict_set:
    one_fas_dict_set.add(item[0])
    one_fas_dict_set.add(item[1])
    one_fas_dict_set.add(item[2])

print(len(smi_dict_set))
print(smi_dict_set)
print(len(fas_dict_set))
print(fas_dict_set)
print(len(one_fas_dict_set))
print(one_fas_dict_set)
print("smilesæœ€å¤§åºåˆ—é•¿åº¦ï¼š",smi_max_len)
print("fastaæœ€å¤§åºåˆ—é•¿åº¦ï¼š",fas_max_len)

smi_dict = {}
fas_dict = {}
count = 1
for item in smi_dict_set:
    smi_dict[item] = count
    count = count + 1
count = 1
for item in fas_dict_set:
    fas_dict[item] = count
    count = count + 1
print(smi_dict)
print(fas_dict)
with open("smi_dict.pickle","wb") as f:
    pickle.dump(smi_dict,f)
with open("fas_dict.pickle","wb") as f:
    pickle.dump(fas_dict,f)
"""
è¯»å–smi_dictçš„æ–¹å¼
with open("smi_dict.pickle","rb") as f:
    test1 = pickle.load(f)
with open("fas_dict.pickle","rb") as f:
    test2 = pickle.load(f)
print(test1)
print(test2)
"""
```

---

## åæŠ˜äº¤å‰è®­ç»ƒæ¨¡å‹ç¡®å®šè¶…å‚æ•°

è¿™é‡Œä¸»è¦ä»‹ç»ç›¸å¯¹äºåŸå§‹çš„åŒ…éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†ã€‚

ç”±äºåŸå§‹çš„åŒ…åªè¾“å‡ºäº†AUCçš„å€¼ï¼Œæ²¡æ³•ä½œå›¾ï¼Œè¿™é‡Œå¢åŠ ä»£ç ç”¨äºè¾“å‡ºéªŒè¯é›†çš„çœŸå®å€¼å’Œé¢„æµ‹å€¼ä»¥ä¾¿ä½œå›¾ã€‚

```python
import time
import datetime
import os

import numpy as np
import torch.optim as optim
import torch
from torch import nn
from torch.autograd import Variable
from torch.nn import functional as F
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score

from config import Config
from utils.util import Helper
from model.CommonModel import Common_model
from model.PredictModel import Predict_model
from dataset import HetrDataset

def train_common_model(config,helper,model,hetrdataset,repeat_nums,flod_nums):

    optimizer = optim.Adam(model.parameters(),config.common_learn_rate)
    model.train()

    print("common model begin training----------",datetime.datetime.now())

    #common_loss
    for e in range(config.common_epochs):

        common_loss = 0
        begin_time = time.time()

        for i, (dg,pt,tag) in enumerate(hetrdataset.get_train_batch(repeat_nums,flod_nums,config.batch_size)):
            dg = helper.to_longtensor(dg,config.use_gpu)
            pt = helper.to_longtensor(pt,config.use_gpu)
            tag = helper.to_floattensor(tag,config.use_gpu)

            optimizer.zero_grad()

            smi_common, fas_common = model(dg,pt)

            distance_loss = helper.comput_distance_loss(smi_common,fas_common,tag)
            common_loss += distance_loss

            distance_loss.backward()
            optimizer.step()

        #end a epoch
        print("the loss of common model epoch[%d / %d]:is %4.f, time:%d s" % (e+1,config.common_epochs,common_loss,time.time()-begin_time))
    return common_loss.item()

def train_predict_model(config,helper,predict_model,common_model,hetrdataset,repeat_nums,flod_nums):

    optimizer1 = optim.Adam(predict_model.parameters(),config.pre_learn_rate)
    optimizer2 = optim.Adam(common_model.parameters(),config.common_learn_rate)

    predict_model.train()
    common_model.train()

    print("predict model begin training----------",datetime.datetime.now())

    #tag_loss
    for e in range(config.predict_epochs):

        epoch_loss = 0
        begin_time = time.time()

        for i, (dg,pt,tag) in enumerate(hetrdataset.get_train_batch(repeat_nums,flod_nums,config.batch_size)):
            dg = helper.to_longtensor(dg,config.use_gpu)
            pt = helper.to_longtensor(pt,config.use_gpu)
            tag = helper.to_floattensor(tag,config.use_gpu)

            smi_common,fas_common = common_model(dg,pt)

            optimizer1.zero_grad()
            optimizer2.zero_grad()

            predict, tag  = predict_model(smi_common,fas_common,tag)

            tag_loss = F.binary_cross_entropy(predict,tag)
            epoch_loss += tag_loss

            tag_loss.backward()

            optimizer1.step()
            optimizer2.step()

        # end a epech
        print("the loss of predict model epoch[%d / %d]:is %4.f, time:%d s" %  (e+1, config.predict_epochs, epoch_loss, time.time() - begin_time))

        #å°†æ¯ä¸€æŠ˜äº¤å‰éªŒè¯çš„æ¨¡å‹å­˜å‚¨èµ·æ¥
        if not os.path.exists('./results'):
            os.mkdir('./results')
        if not os.path.exists('./results/com_model_parm'):
            os.mkdir('./results/com_model_parm')
        if not os.path.exists('./results/pre_model_parm'):
            os.mkdir('./results/pre_model_parm')

        #evaluation_model(config, helper, predict_model, common_model, hetrdataset, repeat_nums, flod_nums)
        if e == config.predict_epochs-1 and epoch == config.num_epochs-1:
            torch.save(common_model.state_dict(),
                       './results/com_model_parm/repeat_%d_corss_%d.parm' % (repeat_nums, flod_nums))
            torch.save(predict_model.state_dict(),
                       './results/pre_model_parm/repeat_%d_corss_%d.parm' % (repeat_nums, flod_nums))
            #è¯„ä¼°æ¨¡å‹
        loss, pre_all, lab_all = evaluation_model(config, helper, predict_model, common_model, hetrdataset, repeat_nums, flod_nums)

    return epoch_loss.item(), loss, pre_all, lab_all

def evaluation_model(config,helper,predict_model,common_model,hetrdataset,repeat_nums,flod_nums):
    predict_model.eval()
    common_model.eval()
    print("evaluate the model")

    begin_time = time.time()
    loss = 0
    avg_acc = []
    avg_aupr = []
    pre_all = []
    lab_all = []
    with torch.no_grad():
        auc_list = []
        auc_list_pre = []
        for i,(dg,pt,tag) in enumerate(hetrdataset.get_test_batch(repeat_nums,flod_nums,config.batch_size)):
            dg = helper.to_longtensor(dg,config.use_gpu)
            pt = helper.to_longtensor(pt,config.use_gpu)
            tag = helper.to_floattensor(tag,config.use_gpu)

            smi_common,fas_common = common_model(dg,pt)
            predict, tag = predict_model(smi_common, fas_common, tag)

            pre_all.append(predict)
            lab_all.append(tag)

            tag_loss = F.binary_cross_entropy(predict,tag)
            loss +=tag_loss

            #print(tag.cpu(),predict.cpu())
            auc_list.append(tag.cpu())
            auc_list_pre.append(predict.cpu())
        #print(auc_list,auc_list_pre)
        auc = roc_auc_score(torch.cat(auc_list,0),torch.cat(auc_list_pre,0))
        aupr = average_precision_score(torch.cat(auc_list,0),torch.cat(auc_list_pre,0))

            #avg_acc.append(auc)
            #avg_aupr.append(aupr)

    print("the total_loss of test model:is %4.f, time:%d s" % (loss, time.time() - begin_time))
    print("avg_acc:",auc,"avg_aupr:",aupr)

    return loss.item(), pre_all, lab_all

if __name__=='__main__':

    # initial parameters class
    config = Config()

    # initial utils class
    helper = Helper()

    #initial data
    hetrdataset = HetrDataset()

    #torch.backends.cudnn.enabled = False æŠŠ

    model_begin_time = time.time()
    for i in range(config.repeat_nums):
        print("repeat:",str(i),"+++++++++++++++++++++++++++++++++++")
        for j in range(config.fold_nums):
            print(" crossfold:", str(j), "----------------------------")
            if not os.path.exists('./results/result_' + str(j) + "/"):
                os.makedirs('./results/result_' + str(j) + "/")
            #initial presentation model
            c_model = Common_model(config)
            p_model = Predict_model()
            if config.use_gpu:
                c_model = c_model.cuda()
                p_model = p_model.cuda()

            loss_rt = "model_loss,predict_loss,val_loss\n"
            for epoch in range(config.num_epochs):
                print("         epoch:",str(epoch),"zzzzzzzzzzzzzzzz")
                common_loss = train_common_model(config,helper,c_model,hetrdataset,i,j)
                predict_loss, val_loss, pre_all, lab_all = train_predict_model(config,helper,p_model,c_model,hetrdataset,i,j)
                loss_rt = loss_rt + str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n"
                print(str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n")
            with open('./results/result_' + str(j) + "/" + "all_loss_" + str(j) + ".csv","w") as rt:
                rt.write(loss_rt)
            pre_lab = "y_pre,y_lab\n"
            # for i in range(len(pre_all)):
            #     pre_lab = pre_lab + str(pre_all[i]) + "," + str(lab_all[i]) + "\n"
            for qq in range(len(pre_all)):
                a = pre_all[qq].tolist()
                b = lab_all[qq].tolist()
                for m in range(len(a)):
                    pre_lab = pre_lab + str(a[m]) + "," + str(b[m]) + "\n"
            with open('./results/result_' + str(j) + "/" + "pre_lab_" + str(j) + ".csv","w") as rt:
                rt.write(pre_lab)

            # with open('./results/result_' + str(j) + "/" + "all_loss_" + str(j) + ".csv","w") as rt:
            #     rt.write("model_loss,predict_loss,val_loss\n")
            #     for epoch in range(config.num_epochs):
            #         print("         epoch:",str(epoch),"zzzzzzzzzzzzzzzz")
            #         common_loss = train_common_model(config,helper,c_model,hetrdataset,i,j)
            #         predict_loss, val_loss, pre_all, lab_all = train_predict_model(config,helper,p_model,c_model,hetrdataset,i,j)
            #         rt.write(str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n")
            #         print(str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n")
            #     pre_lab = "y_pre,y_lab\n"
            #     for i in range(len(pre_all)):
            #         a = pre_all[i].tolist()
            #         b = lab_all[i].tolist()
            #         for m in range(len(a)):
            #             pre_lab = pre_lab + str(a[m]) + "," + str(b[m]) + "\n"
            #     with open('./results/result_' + str(j) + "/" + "pre_lab_" + str(j) + ".csv","w") as rt:
            #         rt.write(pre_lab)

    print("Done!")
    print("All_training time:",time.time()-model_begin_time)
```

---

## åŸºäºæ‰€æœ‰æ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹

ä¿®æ”¹ä¸»ä½“main.pyæ–‡ä»¶ã€‚

```python
import time
import datetime
import os

import numpy as np
import torch.optim as optim
import torch
from torch import nn
from torch.autograd import Variable
from torch.nn import functional as F
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score

from config import Config
from utils.util import Helper
from model.CommonModel import Common_model
from model.PredictModel import Predict_model
from dataset import HetrDataset

def train_common_model(config,helper,model,hetrdataset):

    optimizer = optim.Adam(model.parameters(),config.common_learn_rate)
    model.train()

    print("common model begin training----------",datetime.datetime.now())

    #common_loss
    for e in range(config.common_epochs):

        common_loss = 0
        begin_time = time.time()

        for i, (dg,pt,tag) in enumerate(hetrdataset.get_train_batch(config.batch_size)):
            dg = helper.to_longtensor(dg,config.use_gpu)
            pt = helper.to_longtensor(pt,config.use_gpu)
            tag = helper.to_floattensor(tag,config.use_gpu)

            optimizer.zero_grad()

            smi_common, fas_common = model(dg,pt)

            distance_loss = helper.comput_distance_loss(smi_common,fas_common,tag)
            common_loss += distance_loss

            distance_loss.backward()
            optimizer.step()

        #end a epech
        print("the loss of common model epoch[%d / %d]:is %4.f, time:%d s" % (e+1,config.common_epochs,common_loss,time.time()-begin_time))
    return common_loss.item()

def train_predict_model(config,helper,predict_model,common_model,hetrdataset):

    optimizer1 = optim.Adam(predict_model.parameters(),config.pre_learn_rate)
    optimizer2 = optim.Adam(common_model.parameters(),config.common_learn_rate)

    predict_model.train()
    common_model.train()

    print("predict model begin training----------",datetime.datetime.now())

    #tag_loss
    for e in range(config.predict_epochs):

        epoch_loss = 0
        begin_time = time.time()

        for i, (dg,pt,tag) in enumerate(hetrdataset.get_train_batch(config.batch_size)):
            dg = helper.to_longtensor(dg,config.use_gpu)
            pt = helper.to_longtensor(pt,config.use_gpu)
            tag = helper.to_floattensor(tag,config.use_gpu)

            smi_common,fas_common = common_model(dg,pt)

            optimizer1.zero_grad()
            optimizer2.zero_grad()

            predict, tag  = predict_model(smi_common,fas_common,tag)

            tag_loss = F.binary_cross_entropy(predict,tag)
            epoch_loss += tag_loss

            tag_loss.backward()

            optimizer1.step()
            optimizer2.step()

        # end a epech
        print("the loss of predict model epoch[%d / %d]:is %4.f, time:%d s" %  (e+1, config.predict_epochs, epoch_loss, time.time() - begin_time))

        #å°†æ¯ä¸€æŠ˜äº¤å‰éªŒè¯çš„æ¨¡å‹å­˜å‚¨èµ·æ¥
        if not os.path.exists('./results'):
            os.mkdir('./results')
        if not os.path.exists('./results/com_model_parm'):
            os.mkdir('./results/com_model_parm')
        if not os.path.exists('./results/pre_model_parm'):
            os.mkdir('./results/pre_model_parm')

        #evaluation_model(config, helper, predict_model, common_model, hetrdataset, repeat_nums, flod_nums)
        if e == config.predict_epochs-1 and epoch == config.num_epochs-1:
            torch.save(common_model.state_dict(),
                       './results/com_model_parm/repeat_corss_all.parm')
            torch.save(predict_model.state_dict(),
                       './results/pre_model_parm/repeat_corss_all.parm')
            #è¯„ä¼°æ¨¡å‹
        #loss, pre_all, lab_all = evaluation_model(config, helper, predict_model, common_model, hetrdataset, repeat_nums, flod_nums)

    #return epoch_loss.item(), loss, pre_all, lab_all

def evaluation_model(config,helper,predict_model,common_model,hetrdataset,repeat_nums,flod_nums):
    predict_model.eval()
    common_model.eval()
    print("evaluate the model")

    begin_time = time.time()
    loss = 0
    avg_acc = []
    avg_aupr = []
    pre_all = []
    lab_all = []
    with torch.no_grad():
        for i,(dg,pt,tag) in enumerate(hetrdataset.get_test_batch(repeat_nums,flod_nums,config.batch_size)):
            dg = helper.to_longtensor(dg,config.use_gpu)
            pt = helper.to_longtensor(pt,config.use_gpu)
            tag = helper.to_floattensor(tag,config.use_gpu)

            smi_common,fas_common = common_model(dg,pt)
            predict, tag = predict_model(smi_common, fas_common, tag)

            pre_all.append(predict)
            lab_all.append(tag)

            tag_loss = F.binary_cross_entropy(predict,tag)
            loss +=tag_loss

            auc = roc_auc_score(tag.cpu(),predict.cpu())
            aupr = average_precision_score(tag.cpu(),predict.cpu())

            avg_acc.append(auc)
            avg_aupr.append(aupr)

    print("the total_loss of test model:is %4.f, time:%d s" % (loss, time.time() - begin_time))
    print("avg_acc:",np.mean(avg_acc),"avg_aupr:",np.mean(avg_aupr))

    return loss.item(), pre_all, lab_all

if __name__=='__main__':

    # initial parameters class
    config = Config()

    # initial utils class
    helper = Helper()

    #initial data
    hetrdataset = HetrDataset()

    #torch.backends.cudnn.enabled = False æŠŠ

    model_begin_time = time.time()
    #for i in range(config.repeat_nums):
    #print("repeat:",str(i),"+++++++++++++++++++++++++++++++++++")
        #for j in range(config.fold_nums):
        #    print(" crossfold:", str(j), "----------------------------")
        #    if not os.path.exists('./results/result_' + str(j) + "/"):
        #        os.makedirs('./results/result_' + str(j) + "/")
            #initial presentation model
    c_model = Common_model(config)
    p_model = Predict_model()
    if config.use_gpu:
        c_model = c_model.cuda()
        p_model = p_model.cuda()

        #    loss_rt = "model_loss,predict_loss,val_loss\n"
    for epoch in range(config.num_epochs):
        #        print("         epoch:",str(epoch),"zzzzzzzzzzzzzzzz")
        train_common_model(config,helper,c_model,hetrdataset)
        train_predict_model(config,helper,p_model,c_model,hetrdataset)
        #        loss_rt = loss_rt + str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n"
        #        print(str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n")
        #    with open('./results/result_' + str(j) + "/" + "all_loss_" + str(j) + ".csv","w") as rt:
        #        rt.write(loss_rt)
        #    pre_lab = "y_pre,y_lab\n"
            # for i in range(len(pre_all)):
            #     pre_lab = pre_lab + str(pre_all[i]) + "," + str(lab_all[i]) + "\n"
        #    for qq in range(len(pre_all)):
        #        a = pre_all[qq].tolist()
        #        b = lab_all[qq].tolist()
        #        for m in range(len(a)):
        #            pre_lab = pre_lab + str(a[m]) + "," + str(b[m]) + "\n"
        #    with open('./results/result_' + str(j) + "/" + "pre_lab_" + str(j) + ".csv","w") as rt:
        #        rt.write(pre_lab)

            # with open('./results/result_' + str(j) + "/" + "all_loss_" + str(j) + ".csv","w") as rt:
            #     rt.write("model_loss,predict_loss,val_loss\n")
            #     for epoch in range(config.num_epochs):
            #         print("         epoch:",str(epoch),"zzzzzzzzzzzzzzzz")
            #         common_loss = train_common_model(config,helper,c_model,hetrdataset,i,j)
            #         predict_loss, val_loss, pre_all, lab_all = train_predict_model(config,helper,p_model,c_model,hetrdataset,i,j)
            #         rt.write(str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n")
            #         print(str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n")
            #     pre_lab = "y_pre,y_lab\n"
            #     for i in range(len(pre_all)):
            #         a = pre_all[i].tolist()
            #         b = lab_all[i].tolist()
            #         for m in range(len(a)):
            #             pre_lab = pre_lab + str(a[m]) + "," + str(b[m]) + "\n"
            #     with open('./results/result_' + str(j) + "/" + "pre_lab_" + str(j) + ".csv","w") as rt:
            #         rt.write(pre_lab)


    print("Done!")
    print("All_training time:",time.time()-model_begin_time)
```

ä¿®æ”¹æ•°æ®å‡†å¤‡dataset.pyæ–‡ä»¶ã€‚

```python
import pandas as pd
import numpy as np
import pickle
from sklearn.model_selection import train_test_split,StratifiedKFold
import scipy.io as scio

from config import Config

class HetrDataset():
    def __init__(self):
        config = Config()
        self.repeat_nums = config.repeat_nums
        self.fold_nums = config.fold_nums
        self.neg_samp_ratio = config.neg_samp_ratio

        self.dg_smi_path = config.dg_smiles_path
        self.pt_fas_path = config.pt_fasta_path
        self.smi_dict_path = config.smi_dict_path
        self.fas_dict_path = config.fas_dict_path
        self.smi_ngram = config.smi_n_gram
        self.fas_ngram = config.fas_n_gram
        self.smi_max_len = config.smiles_max_len
        self.fas_max_len = config.fasta_max_len

        self.dg_pt_path = config.dg_pt_path

        self.read_data()
        self.pre_process()

    def read_data(self):
        #è¿™æ˜¯åºåˆ—æ•°æ®ï¼Œè¿˜æ²¡è½¬å˜ä¸ºæ•°å€¼ç‰¹å¾
        self.drug_smi = pd.read_csv(self.dg_smi_path,header=None,index_col=None).values
        self.protein_fas = pd.read_csv(self.pt_fas_path,header=None,index_col=None).values
        #å¯¼å…¥å­—ç¬¦ä¸æ•°å€¼ä¹‹é—´æ˜ å°„çš„å­—å…¸
        with open(self.smi_dict_path, "rb") as f:
            self.smi_dict = pickle.load(f)
        with open(self.fas_dict_path, "rb") as f:
            self.fas_dict = pickle.load(f)

        self.dg_pt = pd.read_csv(self.dg_pt_path, header=0, index_col=0).values


    def pre_process(self):
        """
        :return:all_data_set:list    repeat_nums*fold_nums*3
        """
        #self.all_data_set = []      #[]:repeat_nums*fold_nums*3
        whole_positive_index = []
        whole_negetive_index = []
        for i in range(self.dg_pt.shape[0]):
            for j in range(self.dg_pt.shape[1]):
                if int(self.dg_pt[i, j]) == 1:
                    whole_positive_index.append([i, j])
                elif int(self.dg_pt[i, j]) == 0:
                    whole_negetive_index.append([i, j])

        for x in range(self.repeat_nums):

            #å¯¹è´Ÿ2ä¾‹è¿›è¡Œself.neg_samp_ratioå€æ­£ä¾‹çš„ä¸‹é‡‡æ ·
            negative_sample_index = np.random.choice(np.arange(len(whole_negetive_index)),
                                                     size=self.neg_samp_ratio * len(whole_positive_index),replace=False)
            self.data_set = np.zeros((self.neg_samp_ratio*len(whole_positive_index) + len(negative_sample_index),3), dtype=int)

            count = 0
            for item in whole_positive_index:
                #å¯¹æ­£ä¾‹è¿›è¡Œself.neg_samp_ratioå€è¿‡é‡‡æ ·
                for i in range(self.neg_samp_ratio):
                    self.data_set[count][0] = item[0]
                    self.data_set[count][1] = item[1]
                    self.data_set[count][2] = 1
                    count = count + 1
            for i in negative_sample_index:
                self.data_set[count][0] = whole_negetive_index[i][0]
                self.data_set[count][1] = whole_negetive_index[i][1]
                self.data_set[count][2] = 0
                count = count + 1

            #all_fold_dataset = []
            #rs = np.random.randint(0,1000,1)[0]
            #kf = StratifiedKFold(n_splits=self.fold_nums, shuffle=True, random_state=rs)
            #for train_index, test_index in kf.split(data_set[:,0:2], data_set[:, 2]):
            #    train_data, test_data = data_set[train_index], data_set[test_index]
            #    one_fold_dataset = []
            #    one_fold_dataset.append(train_data)
            #    one_fold_dataset.append(test_data)
            #    all_fold_dataset.append(one_fold_dataset)

            #self.all_data_set.append(all_fold_dataset)

        #å°†åºåˆ—è½¬ä¸ºæ•°å€¼ç‰¹å¾è¡¨ç¤º
        self.smi_input = np.zeros((len(self.drug_smi),self.smi_max_len),dtype=int)
        self.fas_input = np.zeros((len(self.protein_fas),self.fas_max_len),dtype=int)

        for i in range(len(self.drug_smi)):
            for j in range(len(self.drug_smi[i,1]) - self.smi_ngram +1):
                key = self.drug_smi[i,1][j:j + self.smi_ngram]
                self.smi_input[i,j] = self.smi_dict[key]

        for i in range(len(self.protein_fas)):
            for j in range(len(self.protein_fas[i,1]) - self.fas_ngram +1):
                key = self.protein_fas[i,1][j:j + self.fas_ngram]
                self.fas_input[i,j] = self.fas_dict[key]

    def get_train_batch(self,batch_size):

        train_drugs = []
        train_proteins = []
        train_affinity = []
        train_data = self.data_set
        #print(type(train_data))     ndarrayç±»å‹,æ²¡æ‰“ä¹±ï¼Œtagå…ˆ1å0
        for index,(i,j,tag) in enumerate(train_data):
            train_drugs.append(self.smi_input[i])
            train_proteins.append(self.fas_input[j])
            train_affinity.append(tag)

        train_drugs = np.array(train_drugs)
        train_proteins = np.array(train_proteins)
        train_affinity = np.array(train_affinity)

        #æ‰“ä¹±è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾ï¼Œé€šè¿‡æ‰“ä¹±ç´¢å¼•ä»è€Œæ‰“ä¹±æ•°æ®ï¼Œæ•°æ®é‡å¾ˆå¤§æ—¶èƒ½èŠ‚çº¦å†…å­˜ï¼Œå¹¶ä¸”æ¯æ¬¡ç”Ÿæˆçš„æ•°æ®éƒ½ä¸ä¸€æ ·
        data_index = np.arange(len(train_drugs))        #ç”Ÿæˆä¸‹æ ‡
        np.random.shuffle(data_index)
        train_drugs = train_drugs[data_index]
        train_proteins = train_proteins[data_index]
        train_affinity = train_affinity[data_index]

        #è¿­ä»£è¿”å›
        sindex = 0
        eindex = batch_size
        while eindex < len(train_drugs):
            tra_dg_batch = train_drugs[sindex:eindex,:]
            tra_pt_batch = train_proteins[sindex:eindex,:]
            tra_tag_batch = train_affinity[sindex:eindex]

            temp = eindex
            eindex = eindex + batch_size
            sindex = temp
            yield tra_dg_batch,tra_pt_batch,tra_tag_batch

        if eindex >= len(train_drugs):
            tra_dg_batch = train_drugs[sindex:,:]
            tra_pt_batch = train_proteins[sindex:,:]
            tra_tag_batch = train_affinity[sindex:]
            yield tra_dg_batch,tra_pt_batch,tra_tag_batch

    def get_test_batch(self,repeat_nums,flod_nums,batch_size):
        #æµ‹è¯•å¯ä»¥ä¸€æ¬¡æ€§è¾“å…¥å§ï¼Œå¦‚æœå†…å­˜å……è¶³çš„è¯
        train_drugs = []
        train_proteins = []
        train_affinity = []
        train_data = self.all_data_set[repeat_nums][flod_nums][1]
        #print(type(train_data))     ndarrayç±»å‹,æ²¡æ‰“ä¹±ï¼Œtagå…ˆ1å0
        for index,(i,j,tag) in enumerate(train_data):
            train_drugs.append(self.smi_input[i])
            train_proteins.append(self.fas_input[j])
            train_affinity.append(tag)

        train_drugs = np.array(train_drugs)
        train_proteins = np.array(train_proteins)
        train_affinity = np.array(train_affinity)

        #æ‰“ä¹±è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾ï¼Œé€šè¿‡æ‰“ä¹±ç´¢å¼•ä»è€Œæ‰“ä¹±æ•°æ®ï¼Œæ•°æ®é‡å¾ˆå¤§æ—¶èƒ½èŠ‚çº¦å†…å­˜ï¼Œå¹¶ä¸”æ¯æ¬¡ç”Ÿæˆçš„æ•°æ®éƒ½ä¸ä¸€æ ·
        data_index = np.arange(len(train_drugs))        #ç”Ÿæˆä¸‹æ ‡
        np.random.shuffle(data_index)
        train_drugs = train_drugs[data_index]
        train_proteins = train_proteins[data_index]
        train_affinity = train_affinity[data_index]

        #è¿­ä»£è¿”å›
        sindex = 0
        eindex = batch_size
        while eindex < len(train_drugs):
            tra_dg_batch = train_drugs[sindex:eindex,:]
            tra_pt_batch = train_proteins[sindex:eindex,:]
            tra_tag_batch = train_affinity[sindex:eindex]

            temp = eindex
            eindex = eindex + batch_size
            sindex = temp
            yield tra_dg_batch,tra_pt_batch,tra_tag_batch

        if eindex >= len(train_drugs):
            tra_dg_batch = train_drugs[sindex:,:]
            tra_pt_batch = train_proteins[sindex:,:]
            tra_tag_batch = train_affinity[sindex:]
            yield tra_dg_batch,tra_pt_batch,tra_tag_batch

#ht = HetrDataset()
```

---

## æµ‹è¯•æ¨¡å‹

### è·å–å°åˆ†å­å’Œé¶æ ‡è›‹ç™½åœ¨ç©ºé—´ä¸­çš„ä½ç½®

```python
import torch
import numpy as np
import pandas as pd

from config import Config
from utils.util import Helper
from model.CommonModel import Common_model
from dataset import HetrDataset
import sys

def get_commm(num):
    #å¯¼å…¥æ¨¡å‹å‚æ•°
    com_parm = torch.load('./results/com_model_parm/repeat_corss_' + num + '.parm')
    #print(com_parm)

    config = Config()
    helper = Helper()
    com_model = Common_model(config)
    dataset = HetrDataset()

    this_use_gpu = False
    smi_input = dataset.smi_input
    fas_input = dataset.fas_input
    smi_input = helper.to_longtensor(smi_input,this_use_gpu)
    fas_input = helper.to_longtensor(fas_input,this_use_gpu)

    com_model.load_state_dict(com_parm)
    com_model.eval()

    #å°†å¼‚æ„æ•°æ®é›†çš„å…¬å…±ç©ºé—´ä¿å­˜èµ·æ¥
    #æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œä»–ä»¬çš„è¡¨ç¤ºéƒ½æ˜¯å›ºå®šçš„

    #è·å–drugåœ¨å…¬å…±ç©ºé—´çš„è¡¨ç¤º
    batch = 14
    drug_com = None
    sindex = 0
    eindex = batch
    with torch.no_grad():
        while eindex < len(smi_input):
            smi_common, _= com_model(smi_input[sindex:eindex], fas_input[0:batch])
            smi_common = smi_common.numpy()
            if sindex == 0:
                drug_com = smi_common
            else:
                drug_com = np.concatenate((drug_com, smi_common), axis=0)

            temp = eindex
            eindex = eindex + batch
            sindex = temp

        if eindex >= len(smi_input):
            new_batch = len(smi_input) - sindex
            smi_common, _ = com_model(smi_input[sindex:], fas_input[0:new_batch])
            smi_common = smi_common.numpy()
            drug_com = np.concatenate((drug_com, smi_common), axis=0)
    print(len(drug_com))

    #è·å–targetåœ¨å…¬å…±ç©ºé—´çš„è¡¨ç¤º
    batch = 14
    target_com = None
    sindex = 0
    eindex = batch
    with torch.no_grad():
        while eindex < len(fas_input):
            _, fas_common = com_model(smi_input[0:batch], fas_input[sindex:eindex])
            fas_common = fas_common.numpy()
            if sindex == 0:
                target_com = fas_common
            else:
                target_com = np.concatenate((target_com, fas_common), axis=0)

            temp = eindex
            eindex = eindex + batch
            sindex = temp

        if eindex >= len(fas_input):
            new_batch = len(fas_input) - sindex
            _, fas_common= com_model(smi_input[0:new_batch], fas_input[sindex:])
            fas_common = fas_common.numpy()
            target_com = np.concatenate((target_com, fas_common), axis=0)
    print(len(target_com))

    #å› ä¸ºå…¬å…±ç©ºé—´çš„è·å–éœ€è¦ä¸€å®šæ—¶é—´ï¼Œæ‰€æœ‰è¿™é‡Œå„ä¸ªèŠ‚ç‚¹åœ¨å…¬å…±ç©ºé—´çš„è¡¨ç¤ºå­˜å‚¨èµ·æ¥
    dg_pt = pd.read_csv('data/drug_protein.csv',header=0,index_col=0)

    dg_file = pd.DataFrame(drug_com,index=dg_pt._stat_axis.values)
    dg_file.to_csv('results/result_' + num + '/drug_com_' + num + '.csv')

    pt_file = pd.DataFrame(target_com,index=dg_pt.columns.values)
    pt_file.to_csv('results/result_' + num + '/target_com_' + num + '.csv')


def main():
    num = str(sys.argv[1])
    get_commm(num)


if __name__=="__main__":
    main()
```

### ä½¿ç”¨å¤–éƒ¨æµ‹è¯•é›†å»ºç«‹äº¤å‰ç½‘ç»œè¿›è¡Œé¢„æµ‹

```python
import os, sys
import torch
import numpy as np
import pandas as pd
import pickle
from collections import defaultdict

from config import Config
from utils.util import Helper
from model.CommonModel import Common_model
from model.PredictModel import Predict_model
from dataset import HetrDataset


"""
è¿™ä¸ªæ–‡ä»¶çš„åŠŸèƒ½ï¼š
é¶æ ‡é¢„æµ‹åŠè™šæ‹Ÿç­›é€‰
"""

def smiles_read(smiles):
    with open(smiles) as f:
        f1 = f.readlines()
    dict_icv_drug = {}
    for line in f1:
        dict_icv_drug[line.split(",")[0]] = line.split(",")[1].strip()
        #break
    #print(dict_icv_drug_encoding)
    return dict_icv_drug

def protein_read(protein):
    with open(protein) as f:
        f1 = f.readlines()
    dict_target_protein = {}
    for line in f1:
        dict_target_protein[line.split(",")[0]] = line.split(",")[1].strip()
    # print(len(dict_target_protein_encoding["P0C6X7_3C"]))
    # print(len(f1[0]))
    return dict_target_protein

def data_process():
    out_dict_drug = smiles_read("./outtest/drug_smiles.txt")
    out_dict_protein = protein_read("./outtest/protein_fasta.txt")
    target_set = list(out_dict_protein.keys())
    with open("./outtest/drug_protein.txt") as f:
        f1 = f.readlines()
    dti_mutidict = defaultdict(list)
    [dti_mutidict[i.split(",")[0]].append(i.split(",")[1].strip()) for i in f1]
    whole_positive = []
    whole_negetive = []
    for key in dti_mutidict:
        for i in dti_mutidict[key]:
            whole_positive.append([key,i,out_dict_drug[key],out_dict_protein[i],1])
        target_no = target_set[:]
        [target_no.remove(i) for i in dti_mutidict[key]]
        for a in target_no:
            whole_negetive.append([key,a,out_dict_drug[key],out_dict_protein[a],0])
    whole_positive = np.array(whole_positive,dtype=object)
    whole_negetive = np.array(whole_negetive,dtype=object)
    data_set = np.vstack((whole_positive,whole_negetive))
    return data_set

with open('./data/smi_dict.pickle',"rb") as f:
    smi_dict = pickle.load(f)
with open('data/fas_dict.pickle',"rb") as f:
    fas_dict = pickle.load(f)

def load_mod(num):
    #å¯¼å…¥å­—å…¸

    #å¯¼å…¥å¼‚æ„æ•°æ®é›†åœ¨å…¬å…±ç©ºé—´çš„è¡¨ç¤º,å¹¶è½¬æ¢ä¸ºtensoræ ¼å¼
    target_com = pd.read_csv('./results/result_' + num + '/target_com_' + num + '.csv',index_col=0,header=0)
    #print(target_com)
    target_index = target_com._stat_axis.values
    target_com = torch.FloatTensor(target_com.values)
    #print(target_com)

    #å¯¼å…¥æ¨¡å‹å‚æ•°
    com_parm = torch.load('./results/com_model_parm/repeat_0_corss_' + num + '.parm')
    pre_parm = torch.load('./results/pre_model_parm/repeat_0_corss_' + num + '.parm')
    #print(com_parm)
    #print(pre_parm)
    return com_parm, pre_parm

# com_parm, pre_parm = load_mod(num)
config = Config()
helper = Helper()
com_model = Common_model(config)
pre_model = Predict_model()
dataset = HetrDataset()

this_use_gpu = False
smi_input = dataset.smi_input
fas_input = dataset.fas_input
smi_input = helper.to_longtensor(smi_input,this_use_gpu)
fas_input = helper.to_longtensor(fas_input,this_use_gpu)

# com_model.load_state_dict(com_parm)
# pre_model.load_state_dict(pre_parm)
# com_model.eval()
# pre_model.eval()

def transmol_linux(in_name,out_name):
    in_suffix=os.path.splitext(in_name)[1][1:]
    out_suffix=os.path.splitext(out_name)[1][1:]
    if out_suffix == "smi":
        command="obabel -i%s %s -ocan -O %s"%(in_suffix,in_name,out_name)
    else:
        command="obabel -i%s %s -o%s -O %s"%(in_suffix,in_name,out_suffix,out_name)
    os.system(command)

def dg_seq2num(smi):
    #å°†smilesåºåˆ—é€šè¿‡å­—å…¸çš„æ˜ å°„ï¼Œè½¬ä¸ºæ•°å€¼è¡¨ç¤º
    output = np.zeros(config.smiles_max_len, dtype=int)

    for i in range(len(smi) - config.smi_n_gram + 1):
        key = smi[i:i + config.smi_n_gram]
        output[i] = smi_dict[key]
    return output

def tg_seq2sum(fas):
    # å°†smilesåºåˆ—é€šè¿‡å­—å…¸çš„æ˜ å°„ï¼Œè½¬ä¸ºæ•°å€¼è¡¨ç¤º
    output = np.zeros(config.fasta_max_len, dtype=int)

    for j in range(len(fas) - config.fas_n_gram + 1):
        key = fas[j:j + config.fas_n_gram]
        output[j] = fas_dict[key]
    return output

def predict_(smiless):
    temp = seq2num(smiless)
    input = []
    input.append(temp)
    input.append(temp)
    input = np.array(input)
    input = torch.LongTensor(input)
    with torch.no_grad():
        input_common, _= com_model(input,fas_input[0:2])
    tag = 1       #è¿™ä¸ªå¯ä»¥éšä¾¿è®¾å®š
    target_predict = dict()
    with torch.no_grad():
        for i in range(len(target_com)):
            predict_rate,tag = pre_model(input_common,target_com[i],tag)  #predict_rateå°±è¡¨ç¤ºä»–ä»¬ä¹‹é—´æœ‰ç›¸äº’ä½œç”¨çš„å¯èƒ½æ€§å¤§å°
            target_predict[target_index[i]] = predict_rate[0].item()      #é’ˆå¯¹å•ä¸ªæ•°æ®ï¼Œå°†tensorè½¬ä¸ºpythonæ•°æ®ç±»å‹ï¼Œå³æŠŠtensorå»æ‰
    return target_predict

# #å¼‚æ„ç½‘ç»œçš„èŠ‚ç‚¹
# temp_name = "DB00385"
# temp = "c1cc(c2c(c1)C(=O)c1c(C2=O)c(O)c2c(c1O)C[C@](C[C@@H]2O[C@H]1C[C@@H]([C@@H]([C@@H](O1)C)O)NC(=O)C(F)(F)F)(C(=O)COC(=O)CCCC)O)OC"

# temp = seq2num(temp)
# input = []
# input.append(temp)
# input.append(temp)
# input = np.array(input)
# input = torch.LongTensor(input)
# #print(input)
# #å¾—åˆ°è¿™ä¸ªè¯ç‰©åœ¨å…¬å…±ç©ºé—´çš„è¡¨ç¤ºï¼Œä¸ç®¡æ˜¯åŸæœ¬å°±åœ¨å…¬å…±ç©ºé—´å†…çš„ï¼Œè¿˜æ˜¯ä¸åœ¨å…¬å…±ç©ºé—´å†…çš„drug
# with torch.no_grad():
#     input_common, _= com_model(input,fas_input[0:2])
#     #print(input_common)

# #è¿›è¡Œé¶æ ‡é¢„æµ‹,å› ä¸ºè¾“å…¥éœ€è¦2ä¸ªæ ·æœ¬ï¼Œæ‰€ä»¥è¿™é‡Œå°±ç›´æ¥å°†input_commonç›´æ¥è¾“å…¥ï¼Œå¾—åˆ°2ä¸ªç›¸åŒçš„ç»“æœ
# tag = 1       #è¿™ä¸ªå¯ä»¥éšä¾¿è®¾å®š
# target_predict = dict()
# with torch.no_grad():

#     for i in range(len(target_com)):
#         predict_rate,tag = pre_model(input_common,target_com[i],tag)  #predict_rateå°±è¡¨ç¤ºä»–ä»¬ä¹‹é—´æœ‰ç›¸äº’ä½œç”¨çš„å¯èƒ½æ€§å¤§å°
#         target_predict[target_index[i]] = predict_rate[0].item()      #é’ˆå¯¹å•ä¸ªæ•°æ®ï¼Œå°†tensorè½¬ä¸ºpythonæ•°æ®ç±»å‹ï¼Œå³æŠŠtensorå»æ‰

#     # print(target_predict)
#     # sort_predict = sorted(target_predict.items(),key= lambda x:x[1],reverse=True)   #reverseé»˜è®¤ä¸ºFalseï¼ŒæŒ‰ä»å°åˆ°å¤§æ’åº
#     # print(sort_predict)

def read_all_target():
    with open("proname_uniprot.txt") as f:
        f1 = f.readlines()
    uni_pro = {}
    for line in f1:
        uni_pro[line.split(",")[0]] = line.strip().split(",")[1]
    return uni_pro

def result_process(mol_name, uni_pro, target_predict_dict):
    output = "drug,target,possibility\n"
    for key in target_predict_dict:
        output = output + mol_name + "," + uni_pro[key] + "," + str(target_predict_dict[key]) + "\n"
    if not os.path.exists('./result_output'):
        os.mkdir('./result_output')
    with open('./result_output' + "/target_predict.txt","w") as rt:
        rt.write(output)

def pair_predict(drug_name,drug_smi,target_name,target_fas,tag):
    drug_smi = dg_seq2num(drug_smi)
    dg_input = []
    dg_input.append(drug_smi)
    dg_input.append(drug_smi)
    print("dg_input", dg_input)
    dg_input = np.array(dg_input)
    dg_input = torch.LongTensor(dg_input)
    print("dg_input", dg_input)

    target_fas = tg_seq2sum(target_fas)
    tg_input = []
    tg_input.append(target_fas)
    tg_input.append(target_fas)
    tg_input = np.array(tg_input)
    tg_input = torch.LongTensor(tg_input)

    with torch.no_grad():
        dg_common, tg_common= com_model(dg_input,tg_input)
        predict_rate,tag_p = pre_model(dg_common,tg_common,tag)
        # print(drug_name,target_name,tag,predict_rate[0].item())
    return drug_name, target_name, predict_rate[0].item(), tag

def final_process(num):
    out_put = "drug_name,uniprot,y_pre,y_label\n"
    out_set = data_process()
    for i in out_set:
        # print(i[0],i[2],i[1],i[3],i[4])
        drug_name, target_name, predic, label = pair_predict(i[0],i[2],i[1],i[3],i[4])
        out_put = out_put + drug_name + "," + target_name + "," + str(predic) + "," + str(label) + "\n"
    with open("./results/result_" + num + "/outtest_pre_lab.csv", "w") as f:
        f.write(out_put)

def main():
    num = str(sys.argv[1])
    # out_set = data_process()
    com_parm, pre_parm = load_mod(num)
    com_model.load_state_dict(com_parm)
    pre_model.load_state_dict(pre_parm)
    com_model.eval()
    pre_model.eval()
    final_process(num)


if __name__=="__main__":
    main()
```

### ä½¿ç”¨å¤–éƒ¨æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹

```python
import os, sys
import torch
import numpy as np
import pandas as pd
import pickle
from collections import defaultdict

from config import Config
from utils.util import Helper
from model.CommonModel import Common_model
from model.PredictModel import Predict_model
from dataset import HetrDataset


"""
è¿™ä¸ªæ–‡ä»¶çš„åŠŸèƒ½ï¼š
é¶æ ‡é¢„æµ‹åŠè™šæ‹Ÿç­›é€‰
"""

def smiles_read(smiles):
    with open(smiles) as f:
        f1 = f.readlines()
    dict_icv_drug = {}
    for line in f1:
        dict_icv_drug[line.split(",")[0]] = line.split(",")[1].strip()
        #break
    #print(dict_icv_drug_encoding)
    return dict_icv_drug

def protein_read(protein):
    with open(protein) as f:
        f1 = f.readlines()
    dict_target_protein = {}
    for line in f1:
        dict_target_protein[line.split(",")[0]] = line.split(",")[1].strip()
    # print(len(dict_target_protein_encoding["P0C6X7_3C"]))
    # print(len(f1[0]))
    return dict_target_protein

def data_process():
    out_dict_drug = smiles_read("./outtest/drug_smiles.txt")
    out_dict_protein = protein_read("./outtest/protein_fasta.txt")
    target_set = list(out_dict_protein.keys())
    with open("./outtest/drug_protein.txt") as f:
        f1 = f.readlines()
    dataset = []
    for i in range(39):
        dataset.append([f1[i].split(",")[0],f1[i].split(",")[1].strip(),out_dict_drug[f1[i].split(",")[0]],out_dict_protein[f1[i].split(",")[1].strip()],1])
    for i in range(39,78):
        dataset.append([f1[i].split(",")[0],f1[i].split(",")[1].strip(),out_dict_drug[f1[i].split(",")[0]],out_dict_protein[f1[i].split(",")[1].strip()],0])
    #dti_mutidict = defaultdict(list)
    #[dti_mutidict[i.split(",")[0]].append(i.split(",")[1].strip()) for i in f1]
    #whole_positive = []
    #whole_negetive = []
    #for key in dti_mutidict:
    #    for i in dti_mutidict[key]:
    #        whole_positive.append([key,i,out_dict_drug[key],out_dict_protein[i],1])
    #    target_no = target_set[:]
    #    [target_no.remove(i) for i in dti_mutidict[key]]
    #    for a in target_no:
    #        whole_negetive.append([key,a,out_dict_drug[key],out_dict_protein[a],0])
    #whole_positive = np.array(whole_positive,dtype=object)
    #whole_negetive = np.array(whole_negetive,dtype=object)
    #data_set = np.vstack((whole_positive,whole_negetive))
    return dataset

with open('./data/smi_dict.pickle',"rb") as f:
    smi_dict = pickle.load(f)
with open('data/fas_dict.pickle',"rb") as f:
    fas_dict = pickle.load(f)

def load_mod(num):
    #å¯¼å…¥å­—å…¸

    #å¯¼å…¥å¼‚æ„æ•°æ®é›†åœ¨å…¬å…±ç©ºé—´çš„è¡¨ç¤º,å¹¶è½¬æ¢ä¸ºtensoræ ¼å¼
    target_com = pd.read_csv('./results/result_' + num + '/target_com_' + num + '.csv',index_col=0,header=0)
    #print(target_com)
    target_index = target_com._stat_axis.values
    target_com = torch.FloatTensor(target_com.values)
    #print(target_com)

    #å¯¼å…¥æ¨¡å‹å‚æ•°
    com_parm = torch.load('./results/com_model_parm/repeat_corss_' + num + '.parm')
    pre_parm = torch.load('./results/pre_model_parm/repeat_corss_' + num + '.parm')
    #print(com_parm)
    #print(pre_parm)
    return com_parm, pre_parm

# com_parm, pre_parm = load_mod(num)
config = Config()
helper = Helper()
com_model = Common_model(config)
pre_model = Predict_model()
dataset = HetrDataset()

this_use_gpu = False
smi_input = dataset.smi_input
fas_input = dataset.fas_input
smi_input = helper.to_longtensor(smi_input,this_use_gpu)
fas_input = helper.to_longtensor(fas_input,this_use_gpu)

# com_model.load_state_dict(com_parm)
# pre_model.load_state_dict(pre_parm)
# com_model.eval()
# pre_model.eval()

def transmol_linux(in_name,out_name):
    in_suffix=os.path.splitext(in_name)[1][1:]
    out_suffix=os.path.splitext(out_name)[1][1:]
    if out_suffix == "smi":
        command="obabel -i%s %s -ocan -O %s"%(in_suffix,in_name,out_name)
    else:
        command="obabel -i%s %s -o%s -O %s"%(in_suffix,in_name,out_suffix,out_name)
    os.system(command)

def dg_seq2num(smi):
    #å°†smilesåºåˆ—é€šè¿‡å­—å…¸çš„æ˜ å°„ï¼Œè½¬ä¸ºæ•°å€¼è¡¨ç¤º
    output = np.zeros(config.smiles_max_len, dtype=int)

    for i in range(len(smi) - config.smi_n_gram + 1):
        key = smi[i:i + config.smi_n_gram]
        output[i] = smi_dict[key]
    return output

def tg_seq2sum(fas):
    # å°†smilesåºåˆ—é€šè¿‡å­—å…¸çš„æ˜ å°„ï¼Œè½¬ä¸ºæ•°å€¼è¡¨ç¤º
    output = np.zeros(config.fasta_max_len, dtype=int)

    for j in range(len(fas) - config.fas_n_gram + 1):
        key = fas[j:j + config.fas_n_gram]
        output[j] = fas_dict[key]
    return output

def predict_(smiless):
    temp = seq2num(smiless)
    input = []
    input.append(temp)
    input.append(temp)
    input = np.array(input)
    input = torch.LongTensor(input)
    with torch.no_grad():
        input_common, _= com_model(input,fas_input[0:2])
    tag = 1       #è¿™ä¸ªå¯ä»¥éšä¾¿è®¾å®š
    target_predict = dict()
    with torch.no_grad():
        for i in range(len(target_com)):
            predict_rate,tag = pre_model(input_common,target_com[i],tag)  #predict_rateå°±è¡¨ç¤ºä»–ä»¬ä¹‹é—´æœ‰ç›¸äº’ä½œç”¨çš„å¯èƒ½æ€§å¤§å°
            target_predict[target_index[i]] = predict_rate[0].item()      #é’ˆå¯¹å•ä¸ªæ•°æ®ï¼Œå°†tensorè½¬ä¸ºpythonæ•°æ®ç±»å‹ï¼Œå³æŠŠtensorå»æ‰
    return target_predict

# #å¼‚æ„ç½‘ç»œçš„èŠ‚ç‚¹
# temp_name = "DB00385"
# temp = "c1cc(c2c(c1)C(=O)c1c(C2=O)c(O)c2c(c1O)C[C@](C[C@@H]2O[C@H]1C[C@@H]([C@@H]([C@@H](O1)C)O)NC(=O)C(F)(F)F)(C(=O)COC(=O)CCCC)O)OC"

# temp = seq2num(temp)
# input = []
# input.append(temp)
# input.append(temp)
# input = np.array(input)
# input = torch.LongTensor(input)
# #print(input)
# #å¾—åˆ°è¿™ä¸ªè¯ç‰©åœ¨å…¬å…±ç©ºé—´çš„è¡¨ç¤ºï¼Œä¸ç®¡æ˜¯åŸæœ¬å°±åœ¨å…¬å…±ç©ºé—´å†…çš„ï¼Œè¿˜æ˜¯ä¸åœ¨å…¬å…±ç©ºé—´å†…çš„drug
# with torch.no_grad():
#     input_common, _= com_model(input,fas_input[0:2])
#     #print(input_common)

# #è¿›è¡Œé¶æ ‡é¢„æµ‹,å› ä¸ºè¾“å…¥éœ€è¦2ä¸ªæ ·æœ¬ï¼Œæ‰€ä»¥è¿™é‡Œå°±ç›´æ¥å°†input_commonç›´æ¥è¾“å…¥ï¼Œå¾—åˆ°2ä¸ªç›¸åŒçš„ç»“æœ
# tag = 1       #è¿™ä¸ªå¯ä»¥éšä¾¿è®¾å®š
# target_predict = dict()
# with torch.no_grad():

#     for i in range(len(target_com)):
#         predict_rate,tag = pre_model(input_common,target_com[i],tag)  #predict_rateå°±è¡¨ç¤ºä»–ä»¬ä¹‹é—´æœ‰ç›¸äº’ä½œç”¨çš„å¯èƒ½æ€§å¤§å°
#         target_predict[target_index[i]] = predict_rate[0].item()      #é’ˆå¯¹å•ä¸ªæ•°æ®ï¼Œå°†tensorè½¬ä¸ºpythonæ•°æ®ç±»å‹ï¼Œå³æŠŠtensorå»æ‰

#     # print(target_predict)
#     # sort_predict = sorted(target_predict.items(),key= lambda x:x[1],reverse=True)   #reverseé»˜è®¤ä¸ºFalseï¼ŒæŒ‰ä»å°åˆ°å¤§æ’åº
#     # print(sort_predict)

def read_all_target():
    with open("proname_uniprot.txt") as f:
        f1 = f.readlines()
    uni_pro = {}
    for line in f1:
        uni_pro[line.split(",")[0]] = line.strip().split(",")[1]
    return uni_pro

def result_process(mol_name, uni_pro, target_predict_dict):
    output = "drug,target,possibility\n"
    for key in target_predict_dict:
        output = output + mol_name + "," + uni_pro[key] + "," + str(target_predict_dict[key]) + "\n"
    if not os.path.exists('./result_output'):
        os.mkdir('./result_output')
    with open('./result_output' + "/target_predict.txt","w") as rt:
        rt.write(output)

def pair_predict(drug_name,drug_smi,target_name,target_fas,tag):
    drug_smi = dg_seq2num(drug_smi)
    dg_input = []
    dg_input.append(drug_smi)
    dg_input.append(drug_smi)
    dg_input = np.array(dg_input)
    dg_input = torch.LongTensor(dg_input)

    target_fas = tg_seq2sum(target_fas)
    tg_input = []
    tg_input.append(target_fas)
    tg_input.append(target_fas)
    tg_input = np.array(tg_input)
    tg_input = torch.LongTensor(tg_input)

    with torch.no_grad():
        dg_common, tg_common= com_model(dg_input,tg_input)
        predict_rate,tag_p = pre_model(dg_common,tg_common,tag)
        # print(drug_name,target_name,tag,predict_rate[0].item())
    return drug_name, target_name, predict_rate[0].item(), tag

def final_process(num):
    out_put = "drug_name,uniprot,y_pre,y_label\n"
    out_set = data_process()
    for i in out_set:
        # print(i[0],i[2],i[1],i[3],i[4])
        drug_name, target_name, predic, label = pair_predict(i[0],i[2],i[1],i[3],i[4])
        out_put = out_put + drug_name + "," + target_name + "," + str(predic) + "," + str(label) + "\n"
    with open("./results/result_" + num + "/outtest_pre_lab.csv", "w") as f:
        f.write(out_put)

def main():
    num = str(sys.argv[1])
    # out_set = data_process()
    com_parm, pre_parm = load_mod(num)
    com_model.load_state_dict(com_parm)
    pre_model.load_state_dict(pre_parm)
    com_model.eval()
    pre_model.eval()
    final_process(num)


if __name__=="__main__":
    main()
```

## ç½‘ç«™æ¥å£

è¯´æ˜ï¼šmultiDTIæ²¡æœ‰åŒ…è£…çš„å¾ˆå¥½ï¼Œéœ€è¦çš„æ–‡ä»¶æ¯”è¾ƒç¹æ‚ï¼Œè¦ç¡®ä¿æ–‡ä»¶é…ç½®å¥½æ–¹èƒ½æ­£å¸¸è¿è¡Œã€‚

é¦–å…ˆéœ€è¦è·å–è¯ç‰©å’Œé¶æ ‡åœ¨ç©ºé—´ä¸­çš„è¡¨ç¤ºå¹¶å­˜å‚¨ä¸‹æ¥ï¼Œç•™ä½œåç”¨ã€‚å…¶æ¬¡åˆ™æ˜¯é¢„æµ‹ã€‚

```python
import os, sys
import torch
import numpy as np
import pandas as pd
import pickle

from utils.config import Config
from utils.util import Helper
from utils.CommonModel import Common_model
from utils.PredictModel import Predict_model
from utils.dataset import HetrDataset

"""
è¿™ä¸ªæ–‡ä»¶çš„åŠŸèƒ½ï¼š
é¶æ ‡é¢„æµ‹åŠè™šæ‹Ÿç­›é€‰
"""
os.environ["CUDA_VISIBLE_DEVICES"] = '1'
path = "/home/yqyang/D3Deep/target_predict_and_vs/target_predict/common_space/main_file/"

#å¯¼å…¥å­—å…¸
with open(path + 'smi_dict.pickle',"rb") as f:
    smi_dict = pickle.load(f)

#å¯¼å…¥å¼‚æ„æ•°æ®é›†åœ¨å…¬å…±ç©ºé—´çš„è¡¨ç¤º,å¹¶è½¬æ¢ä¸ºtensoræ ¼å¼
target_com = pd.read_csv(path + 'target_com_all.csv',index_col=0,header=0)
#print(target_com)
target_index = target_com._stat_axis.values
target_com = torch.FloatTensor(target_com.values)
#print(target_com)

#å¯¼å…¥æ¨¡å‹å‚æ•°
com_parm = torch.load(path + 'repeat_0_corss_2_com.parm')
pre_parm = torch.load(path + 'repeat_0_corss_2_pre.parm')
#print(com_parm)
#print(pre_parm)

config = Config()
helper = Helper()
com_model = Common_model(config)
pre_model = Predict_model()
dataset = HetrDataset()

this_use_gpu = False
smi_input = dataset.smi_input
fas_input = dataset.fas_input
smi_input = helper.to_longtensor(smi_input,this_use_gpu)
fas_input = helper.to_longtensor(fas_input,this_use_gpu)

com_model.load_state_dict(com_parm)
pre_model.load_state_dict(pre_parm)
com_model.eval()
pre_model.eval()

def transmol_linux(in_name,out_name):
    in_suffix=os.path.splitext(in_name)[1][1:]
    out_suffix=os.path.splitext(out_name)[1][1:]
    if out_suffix == "smi":
        command="obabel -i%s %s -ocan -O %s"%(in_suffix,in_name,out_name)
    else:
        command="obabel -i%s %s -o%s -O %s"%(in_suffix,in_name,out_suffix,out_name)
    os.system(command)

def seq2num(smi):
    #å°†smilesåºåˆ—é€šè¿‡å­—å…¸çš„æ˜ å°„ï¼Œè½¬ä¸ºæ•°å€¼è¡¨ç¤º
    output = np.zeros(config.smiles_max_len, dtype=int)

    for i in range(len(smi) - config.smi_n_gram + 1):
        key = smi[i:i + config.smi_n_gram]
        output[i] = smi_dict[key]
    return output

def predict_(smiless):
    temp = seq2num(smiless)
    input = []
    input.append(temp)
    input.append(temp)
    input = np.array(input)
    input = torch.LongTensor(input)
    with torch.no_grad():
        input_common, _= com_model(input,fas_input[0:2])
    tag = 1       #è¿™ä¸ªå¯ä»¥éšä¾¿è®¾å®š
    target_predict = dict()
    with torch.no_grad():
        for i in range(len(target_com)):
            predict_rate,tag = pre_model(input_common,target_com[i],tag)  #predict_rateå°±è¡¨ç¤ºä»–ä»¬ä¹‹é—´æœ‰ç›¸äº’ä½œç”¨çš„å¯èƒ½æ€§å¤§å°
            target_predict[target_index[i]] = predict_rate[0].item()      #é’ˆå¯¹å•ä¸ªæ•°æ®ï¼Œå°†tensorè½¬ä¸ºpythonæ•°æ®ç±»å‹ï¼Œå³æŠŠtensorå»æ‰
    return target_predict

# #å¼‚æ„ç½‘ç»œçš„èŠ‚ç‚¹
# temp_name = "DB00385"
# temp = "c1cc(c2c(c1)C(=O)c1c(C2=O)c(O)c2c(c1O)C[C@](C[C@@H]2O[C@H]1C[C@@H]([C@@H]([C@@H](O1)C)O)NC(=O)C(F)(F)F)(C(=O)COC(=O)CCCC)O)OC"

# temp = seq2num(temp)
# input = []
# input.append(temp)
# input.append(temp)
# input = np.array(input)
# input = torch.LongTensor(input)
# #print(input)
# #å¾—åˆ°è¿™ä¸ªè¯ç‰©åœ¨å…¬å…±ç©ºé—´çš„è¡¨ç¤ºï¼Œä¸ç®¡æ˜¯åŸæœ¬å°±åœ¨å…¬å…±ç©ºé—´å†…çš„ï¼Œè¿˜æ˜¯ä¸åœ¨å…¬å…±ç©ºé—´å†…çš„drug
# with torch.no_grad():
#     input_common, _= com_model(input,fas_input[0:2])
#     #print(input_common)

# #è¿›è¡Œé¶æ ‡é¢„æµ‹,å› ä¸ºè¾“å…¥éœ€è¦2ä¸ªæ ·æœ¬ï¼Œæ‰€ä»¥è¿™é‡Œå°±ç›´æ¥å°†input_commonç›´æ¥è¾“å…¥ï¼Œå¾—åˆ°2ä¸ªç›¸åŒçš„ç»“æœ
# tag = 1       #è¿™ä¸ªå¯ä»¥éšä¾¿è®¾å®š
# target_predict = dict()
# with torch.no_grad():

#     for i in range(len(target_com)):
#         predict_rate,tag = pre_model(input_common,target_com[i],tag)  #predict_rateå°±è¡¨ç¤ºä»–ä»¬ä¹‹é—´æœ‰ç›¸äº’ä½œç”¨çš„å¯èƒ½æ€§å¤§å°
#         target_predict[target_index[i]] = predict_rate[0].item()      #é’ˆå¯¹å•ä¸ªæ•°æ®ï¼Œå°†tensorè½¬ä¸ºpythonæ•°æ®ç±»å‹ï¼Œå³æŠŠtensorå»æ‰

#     # print(target_predict)
#     # sort_predict = sorted(target_predict.items(),key= lambda x:x[1],reverse=True)   #reverseé»˜è®¤ä¸ºFalseï¼ŒæŒ‰ä»å°åˆ°å¤§æ’åº
#     # print(sort_predict)

def read_all_target():
    with open(path + "proname_uniprot.txt") as f:
        f1 = f.readlines()
    uni_pro = {}
    for line in f1:
        uni_pro[line.split(",")[0]] = line.strip().split(",")[1]
    return uni_pro

def result_process(mol_name, uni_pro, target_predict_dict):
    output = "drug,target,possibility\n"
    for key in target_predict_dict:
        output = output + mol_name + "," + uni_pro[key] + "," + str(target_predict_dict[key]) + "\n"
    if not os.path.exists('./result_output'):
        os.mkdir('./result_output')
    with open('./result_output' + "/target_predict.txt","w") as rt:
        rt.write(output)

def main():
    mol_file = str(sys.argv[1])
    mol_name = os.path.splitext(mol_file)[0]
    transmol_linux(mol_file, mol_name + ".smi")
    with open(mol_name + ".smi") as f:
        smiles = f.readline().strip().split("\t")[0]
    # target_name, target_fasta = read_target()
    # target_predict(smiles, mol_name, target_name, target_fasta)
    print(smiles)
    target_predict_dict = predict_(smiles)
    uni_pro = read_all_target()
    result_process(mol_name, uni_pro, target_predict_dict)

if __name__=="__main__":
    main()
```

## æ•°æ®åˆ†æ

ï¼ˆ1ï¼‰ç»˜åˆ¶lossæ›²çº¿

å‚è€ƒï¼š[Pythonç»˜å›¾|Pythonç»˜åˆ¶å¤šæ›²çº¿å›¾](./Pythonç»˜å›¾Pythonç»˜åˆ¶å¤šæ›²çº¿å›¾.md)

ï¼ˆ2ï¼‰ç»˜åˆ¶ROCå’ŒPRæ›²çº¿

å‚è€ƒï¼š[Pythonç»˜å›¾|Pythonç»˜åˆ¶ROCæ›²çº¿å’ŒPRæ›²çº¿](./Pythonç»˜å›¾Pythonç»˜åˆ¶ROCæ›²çº¿å’ŒPRæ›²çº¿.md)

ï¼ˆ3ï¼‰è®¡ç®—AUCã€AUPRCã€Accã€Preã€F1ç­‰æŒ‡æ ‡

å‚è€ƒï¼š

## å…¶ä»–

### ç»­è·‘

### å¤šGPUè®­ç»ƒã€é¢„æµ‹

#### è®­ç»ƒ

```python
import time
import datetime
import os
import copy

import numpy as np
import torch.optim as optim
import torch
from torch import nn
from torch.autograd import Variable
from torch.nn import functional as F
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score

from config import Config
from utils.util import Helper
from MPNN.CommonModel import Common_model
from MPNN.PredictModel import Predict_model
from dataset import HetrDataset

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def train_common_model(config,helper,model,hetrdataset,repeat_nums,flod_nums):

    optimizer = optim.Adam(model.parameters(),config.common_learn_rate)
    model.train()

    print("common model begin training----------",datetime.datetime.now())

    #common_loss
    for e in range(config.common_epochs):

        common_loss = 0
        begin_time = time.time()

        for i, (dg,pt,tag) in enumerate(hetrdataset.get_train_batch(repeat_nums,flod_nums,config.batch_size)):
            #dg = helper.to_longtensor(dg,config.use_gpu)
            pt = helper.to_longtensor(pt,config.use_gpu)
            tag = helper.to_floattensor(tag,config.use_gpu)

            optimizer.zero_grad()

            smi_common, fas_common = model(dg,pt)

            distance_loss = helper.comput_distance_loss(smi_common,fas_common,tag)
            common_loss += distance_loss

            distance_loss.backward()
            optimizer.step()

        #end a epech
        print("the loss of common model epoch[%d / %d]:is %4.f, time:%d s" % (e+1,config.common_epochs,common_loss,time.time()-begin_time))
    return common_loss.item()

def train_predict_model(config,helper,predict_model,common_model,hetrdataset,repeat_nums,flod_nums):

    optimizer1 = optim.Adam(predict_model.parameters(),config.pre_learn_rate)
    optimizer2 = optim.Adam(common_model.parameters(),config.common_learn_rate)

    predict_model.train()
    common_model.train()

    print("predict model begin training----------",datetime.datetime.now())

    #tag_loss
    for e in range(config.predict_epochs):

        max_auc = 0
        epoch_loss = 0
        begin_time = time.time()

        for i, (dg,pt,tag) in enumerate(hetrdataset.get_train_batch(repeat_nums,flod_nums,config.batch_size)):
            #dg = helper.to_longtensor(dg,config.use_gpu)
            pt = helper.to_longtensor(pt,config.use_gpu)
            tag = helper.to_floattensor(tag,config.use_gpu)

            smi_common,fas_common = common_model(dg,pt)

            optimizer1.zero_grad()
            optimizer2.zero_grad()

            predict, tag  = predict_model(smi_common,fas_common,tag)

            tag_loss = F.binary_cross_entropy(predict,tag)
            epoch_loss += tag_loss

            tag_loss.backward()

            optimizer1.step()
            optimizer2.step()

        _, _, _, auc = evaluation_model(config, helper, predict_model, common_model, hetrdataset, repeat_nums, flod_nums)

        if auc > max_auc:
            model_max_p = copy.deepcopy(predict_model)
            model_max_c = copy.deepcopy(common_model)
            max_auc = auc

        # end a epech
        print("the loss of predict model epoch[%d / %d]:is %4.f, time:%d s" %  (e+1, config.predict_epochs, epoch_loss, time.time() - begin_time))

    # #å°†æ¯ä¸€æŠ˜äº¤å‰éªŒè¯çš„æ¨¡å‹å­˜å‚¨èµ·æ¥
    # if not os.path.exists('./results'):
    #     os.mkdir('./results')
    # if not os.path.exists('./results/com_model_parm'):
    #     os.mkdir('./results/com_model_parm')
    # if not os.path.exists('./results/pre_model_parm'):
    #     os.mkdir('./results/pre_model_parm')

    #evaluation_model(config, helper, predict_model, common_model, hetrdataset, repeat_nums, flod_nums)
    #if e == config.predict_epochs-1 and epoch == config.num_epochs-1:
    # torch.save(common_model.state_dict(),
    #            './results/com_model_parm/repeat_%d_corss_%d.parm' % (repeat_nums, flod_nums))
    # torch.save(model_max.state_dict(),
    #            './results/pre_model_parm/repeat_%d_corss_%d.parm' % (repeat_nums, flod_nums))
    #è¯„ä¼°æ¨¡å‹

    loss, pre_all, lab_all, _ = evaluation_model(config, helper, model_max_p, model_max_c, hetrdataset, repeat_nums, flod_nums)

    return epoch_loss.item(), loss, pre_all, lab_all, model_max_p, model_max_c

def evaluation_model(config,helper,predict_model,common_model,hetrdataset,repeat_nums,flod_nums):
    predict_model.eval()
    common_model.eval()
    print("evaluate the model")

    begin_time = time.time()
    loss = 0
    avg_acc = []
    avg_aupr = []
    pre_all = []
    lab_all = []
    with torch.no_grad():
        for i,(dg,pt,tag) in enumerate(hetrdataset.get_test_batch(repeat_nums,flod_nums,config.batch_size)):
            #dg = helper.to_longtensor(dg,config.use_gpu)
            pt = helper.to_longtensor(pt,config.use_gpu)
            tag = helper.to_floattensor(tag,config.use_gpu)

            smi_common,fas_common = common_model(dg,pt)
            predict, tag = predict_model(smi_common, fas_common, tag)

            pre_all.append(predict)
            lab_all.append(tag)

            tag_loss = F.binary_cross_entropy(predict,tag)
            loss +=tag_loss

            auc = roc_auc_score(tag.cpu(),predict.cpu())
            aupr = average_precision_score(tag.cpu(),predict.cpu())

            avg_acc.append(auc)
            avg_aupr.append(aupr)

    print("the total_loss of test model:is %4.f, time:%d s" % (loss, time.time() - begin_time))
    print("avg_acc:",np.mean(avg_acc),"avg_aupr:",np.mean(avg_aupr))

    return loss.item(), pre_all, lab_all, np.mean(avg_acc)

if __name__=='__main__':

    # initial parameters class
    config = Config()

    # initial utils class
    helper = Helper()

    #initial data
    hetrdataset = HetrDataset()

    #torch.backends.cudnn.enabled = False æŠŠ

    model_begin_time = time.time()
    for i in range(config.repeat_nums):
        print("repeat:",str(i),"+++++++++++++++++++++++++++++++++++")
        for j in range(config.fold_nums):
            print(" crossfold:", str(j), "----------------------------")
            if not os.path.exists('./results/result_' + str(j) + "/"):
                os.makedirs('./results/result_' + str(j) + "/")
            #initial presentation model
            c_model = Common_model(config)
            p_model = Predict_model()
            if config.use_gpu:
                if torch.cuda.device_count() > 1:
                    #c_model = c_model.cuda()
                    #p_model = p_model.cuda()
                    c_model = c_model.to(device)
                    p_model = p_model.to(device)
                    c_model = nn.DataParallel(c_model, dim = 0)
                    p_model = nn.DataParallel(p_model, dim = 0)

            loss_rt = "model_loss,predict_loss,val_loss\n"
            maxx_auc = 0
            for epoch in range(config.num_epochs):
                print("         epoch:",str(epoch),"zzzzzzzzzzzzzzzz")
                common_loss = train_common_model(config,helper,c_model,hetrdataset,i,j)
                predict_loss, val_loss, pre_all, lab_all, pred_model, com_model = train_predict_model(config,helper,p_model,c_model,hetrdataset,i,j)
                if not os.path.exists('./results'):
                    os.mkdir('./results')
                if not os.path.exists('./results/com_model_parm'):
                    os.mkdir('./results/com_model_parm')
                if not os.path.exists('./results/pre_model_parm'):
                    os.mkdir('./results/pre_model_parm')
                _, _, _, auc = evaluation_model(config, helper, pred_model, com_model, hetrdataset, i, j)
                if auc > maxx_auc:
                    model_train_max = copy.deepcopy(com_model)
                    model_pre_max = copy.deepcopy(pred_model)
                    max_auc = auc
                loss_rt = loss_rt + str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n"
                print(str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n")
            torch.save(model_train_max.state_dict(),
                       './results/com_model_parm/repeat_%d_corss_%d.parm' % (i, j))
            torch.save(model_pre_max.state_dict(),
                       './results/pre_model_parm/repeat_%d_corss_%d.parm' % (i, j))
            with open('./results/result_' + str(j) + "/" + "all_loss_" + str(j) + ".csv","w") as rt:
                rt.write(loss_rt)
            predict_loss, val_loss, pre_all, lab_all, _, _ = train_predict_model(config,helper,model_pre_max,model_train_max,hetrdataset,i,j)
            pre_lab = "y_pre,y_lab\n"
            # for i in range(len(pre_all)):
            #     pre_lab = pre_lab + str(pre_all[i]) + "," + str(lab_all[i]) + "\n"
            for qq in range(len(pre_all)):
                a = pre_all[qq].tolist()
                b = lab_all[qq].tolist()
                for m in range(len(a)):
                    pre_lab = pre_lab + str(a[m]) + "," + str(b[m]) + "\n"
            with open('./results/result_' + str(j) + "/" + "pre_lab_" + str(j) + ".csv","w") as rt:
                rt.write(pre_lab)

            # with open('./results/result_' + str(j) + "/" + "all_loss_" + str(j) + ".csv","w") as rt:
            #     rt.write("model_loss,predict_loss,val_loss\n")
            #     for epoch in range(config.num_epochs):
            #         print("         epoch:",str(epoch),"zzzzzzzzzzzzzzzz")
            #         common_loss = train_common_model(config,helper,c_model,hetrdataset,i,j)
            #         predict_loss, val_loss, pre_all, lab_all = train_predict_model(config,helper,p_model,c_model,hetrdataset,i,j)
            #         rt.write(str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n")
            #         print(str(common_loss) + "," + str(predict_loss) + "," + str(val_loss) + "\n")
            #     pre_lab = "y_pre,y_lab\n"
            #     for i in range(len(pre_all)):
            #         a = pre_all[i].tolist()
            #         b = lab_all[i].tolist()
            #         for m in range(len(a)):
            #             pre_lab = pre_lab + str(a[m]) + "," + str(b[m]) + "\n"
            #     with open('./results/result_' + str(j) + "/" + "pre_lab_" + str(j) + ".csv","w") as rt:
            #         rt.write(pre_lab)


    print("Done!")
    print("All_training time:",time.time()-model_begin_time)
```

#### é¢„æµ‹

```python
import os, sys
import torch
import numpy as np
import pandas as pd
import pickle
from collections import defaultdict

from config import Config
from utils.util import Helper
from MPNN.CommonModel import Common_model
from MPNN.PredictModel import Predict_model
from dataset import HetrDataset

from MPNN.utils import *

"""
è¿™ä¸ªæ–‡ä»¶çš„åŠŸèƒ½ï¼š
é¶æ ‡é¢„æµ‹åŠè™šæ‹Ÿç­›é€‰
"""

config = Config()
helper = Helper()
com_model = Common_model(config)
pre_model = Predict_model()
dataset = HetrDataset()

def smiles_read(smiles):
    with open(smiles) as f:
        f1 = f.readlines()
    dict_icv_drug = {}
    for line in f1:
        dict_icv_drug[line.split(",")[0]] = line.split(",")[1].strip()
        #break
    #print(dict_icv_drug_encoding)
    return dict_icv_drug

def protein_read(protein):
    with open(protein) as f:
        f1 = f.readlines()
    dict_target_protein = {}
    for line in f1:
        dict_target_protein[line.split(",")[0]] = line.split(",")[1].strip()
    # print(len(dict_target_protein_encoding["P0C6X7_3C"]))
    # print(len(f1[0]))
    return dict_target_protein

def data_process():
    out_dict_drug = smiles_read("./outtest/drug_smiles.txt")
    out_dict_protein = protein_read("./outtest/protein_fasta.txt")
    target_set = list(out_dict_protein.keys())
    with open("./outtest/drug_protein.txt") as f:
        f1 = f.readlines()
    dti_mutidict = defaultdict(list)
    [dti_mutidict[i.split(",")[0]].append(i.split(",")[1].strip()) for i in f1]
    whole_positive = []
    whole_negetive = []
    for key in dti_mutidict:
        for i in dti_mutidict[key]:
            whole_positive.append([key,i,out_dict_drug[key],out_dict_protein[i],1])
        target_no = target_set[:]
        [target_no.remove(i) for i in dti_mutidict[key]]
        for a in target_no:
            whole_negetive.append([key,a,out_dict_drug[key],out_dict_protein[a],0])
    whole_positive = np.array(whole_positive,dtype=object)
    whole_negetive = np.array(whole_negetive,dtype=object)
    data_set = np.vstack((whole_positive,whole_negetive))
    return data_set

# with open('./data/smi_dict.pickle',"rb") as f:
#     smi_dict = pickle.load(f)
with open('data/fas_dict.pickle',"rb") as f:
    fas_dict = pickle.load(f)

def load_mod(num):
    #å¯¼å…¥å­—å…¸

    #å¯¼å…¥å¼‚æ„æ•°æ®é›†åœ¨å…¬å…±ç©ºé—´çš„è¡¨ç¤º,å¹¶è½¬æ¢ä¸ºtensoræ ¼å¼
    target_com = pd.read_csv('./results/result_' + num + '/target_com_' + num + '.csv',index_col=0,header=0)
    #print(target_com)
    target_index = target_com._stat_axis.values
    target_com = torch.FloatTensor(target_com.values)
    #print(target_com)

    #å¯¼å…¥æ¨¡å‹å‚æ•°
    com_parm = torch.load('./results/com_model_parm/repeat_0_corss_' + num + '.parm')

    if next(iter(com_parm))[:7] == 'module.':
        # the pretrained model is from data-parallel module
        from collections import OrderedDict
        new_state_dict = OrderedDict()
        for k, v in com_parm.items():
            name = k[7:] # remove `module.`
            new_state_dict[name] = v
        com_parm = new_state_dict

    pre_parm = torch.load('./results/pre_model_parm/repeat_0_corss_' + num + '.parm')

    if next(iter(pre_parm))[:7] == 'module.':
        # the pretrained model is from data-parallel module
        from collections import OrderedDict
        new_state_dict = OrderedDict()
        for k, v in pre_parm.items():
            name = k[7:] # remove `module.`
            new_state_dict[name] = v
        pre_parm = new_state_dict

    #print(com_parm)
    #print(pre_parm)
    return com_parm, pre_parm

# com_parm, pre_parm = load_mod(num)
# config = Config()
# helper = Helper()
# com_model = Common_model(config)
# pre_model = Predict_model()
# dataset = HetrDataset()

this_use_gpu = True
# smi_input = dataset.smi_input
fas_input = dataset.fas_input
# smi_input = helper.to_longtensor(smi_input,this_use_gpu)
fas_input = helper.to_longtensor(fas_input,this_use_gpu)

# com_model.load_state_dict(com_parm)
# pre_model.load_state_dict(pre_parm)
# com_model.eval()
# pre_model.eval()

def transmol_linux(in_name,out_name):
    in_suffix=os.path.splitext(in_name)[1][1:]
    out_suffix=os.path.splitext(out_name)[1][1:]
    if out_suffix == "smi":
        command="obabel -i%s %s -ocan -O %s"%(in_suffix,in_name,out_name)
    else:
        command="obabel -i%s %s -o%s -O %s"%(in_suffix,in_name,out_suffix,out_name)
    os.system(command)

def dg_seq2num(smi):
    #å°†smilesåºåˆ—é€šè¿‡å­—å…¸çš„æ˜ å°„ï¼Œè½¬ä¸ºæ•°å€¼è¡¨ç¤º
    # output = np.zeros(config.smiles_max_len, dtype=int)

    # for i in range(len(smi) - config.smi_n_gram + 1):
    #     key = smi[i:i + config.smi_n_gram]
    #     output[i] = smi_dict[key]
    fatoms = []
    fbonds = []
    agraph = []
    bgraph = []
    shape_tensor = []

    [a, b, c, d, e] = smiles2mpnnfeature(smi)
    fatoms.append(a)
    fbonds.append(b)
    agraph.append(c)
    bgraph.append(d)
    shape_tensor.append(e)

    v_d = []
    v_d.append(torch.stack(fatoms))
    v_d.append(torch.stack(fbonds))
    v_d.append(torch.stack(agraph))
    v_d.append(torch.stack(bgraph))
    v_d.append(torch.stack(shape_tensor).squeeze(1))
    return v_d

def tg_seq2sum(fas):
    # å°†smilesåºåˆ—é€šè¿‡å­—å…¸çš„æ˜ å°„ï¼Œè½¬ä¸ºæ•°å€¼è¡¨ç¤º
    output = np.zeros(config.fasta_max_len, dtype=int)

    for j in range(len(fas) - config.fas_n_gram + 1):
        key = fas[j:j + config.fas_n_gram]
        output[j] = fas_dict[key]
    return output

def predict_(smiless):
    temp = seq2num(smiless)
    input = []
    input.append(temp)
    input.append(temp)
    input = np.array(input)
    input = torch.LongTensor(input)
    with torch.no_grad():
        input_common, _= com_model(input,fas_input[0:2])
    tag = 1       #è¿™ä¸ªå¯ä»¥éšä¾¿è®¾å®š
    target_predict = dict()
    with torch.no_grad():
        for i in range(len(target_com)):
            predict_rate,tag = pre_model(input_common,target_com[i],tag)  #predict_rateå°±è¡¨ç¤ºä»–ä»¬ä¹‹é—´æœ‰ç›¸äº’ä½œç”¨çš„å¯èƒ½æ€§å¤§å°
            target_predict[target_index[i]] = predict_rate[0].item()      #é’ˆå¯¹å•ä¸ªæ•°æ®ï¼Œå°†tensorè½¬ä¸ºpythonæ•°æ®ç±»å‹ï¼Œå³æŠŠtensorå»æ‰
    return target_predict

# #å¼‚æ„ç½‘ç»œçš„èŠ‚ç‚¹
# temp_name = "DB00385"
# temp = "c1cc(c2c(c1)C(=O)c1c(C2=O)c(O)c2c(c1O)C[C@](C[C@@H]2O[C@H]1C[C@@H]([C@@H]([C@@H](O1)C)O)NC(=O)C(F)(F)F)(C(=O)COC(=O)CCCC)O)OC"

# temp = seq2num(temp)
# input = []
# input.append(temp)
# input.append(temp)
# input = np.array(input)
# input = torch.LongTensor(input)
# #print(input)
# #å¾—åˆ°è¿™ä¸ªè¯ç‰©åœ¨å…¬å…±ç©ºé—´çš„è¡¨ç¤ºï¼Œä¸ç®¡æ˜¯åŸæœ¬å°±åœ¨å…¬å…±ç©ºé—´å†…çš„ï¼Œè¿˜æ˜¯ä¸åœ¨å…¬å…±ç©ºé—´å†…çš„drug
# with torch.no_grad():
#     input_common, _= com_model(input,fas_input[0:2])
#     #print(input_common)

# #è¿›è¡Œé¶æ ‡é¢„æµ‹,å› ä¸ºè¾“å…¥éœ€è¦2ä¸ªæ ·æœ¬ï¼Œæ‰€ä»¥è¿™é‡Œå°±ç›´æ¥å°†input_commonç›´æ¥è¾“å…¥ï¼Œå¾—åˆ°2ä¸ªç›¸åŒçš„ç»“æœ
# tag = 1       #è¿™ä¸ªå¯ä»¥éšä¾¿è®¾å®š
# target_predict = dict()
# with torch.no_grad():

#     for i in range(len(target_com)):
#         predict_rate,tag = pre_model(input_common,target_com[i],tag)  #predict_rateå°±è¡¨ç¤ºä»–ä»¬ä¹‹é—´æœ‰ç›¸äº’ä½œç”¨çš„å¯èƒ½æ€§å¤§å°
#         target_predict[target_index[i]] = predict_rate[0].item()      #é’ˆå¯¹å•ä¸ªæ•°æ®ï¼Œå°†tensorè½¬ä¸ºpythonæ•°æ®ç±»å‹ï¼Œå³æŠŠtensorå»æ‰

#     # print(target_predict)
#     # sort_predict = sorted(target_predict.items(),key= lambda x:x[1],reverse=True)   #reverseé»˜è®¤ä¸ºFalseï¼ŒæŒ‰ä»å°åˆ°å¤§æ’åº
#     # print(sort_predict)

def read_all_target():
    with open("proname_uniprot.txt") as f:
        f1 = f.readlines()
    uni_pro = {}
    for line in f1:
        uni_pro[line.split(",")[0]] = line.strip().split(",")[1]
    return uni_pro

def result_process(mol_name, uni_pro, target_predict_dict):
    output = "drug,target,possibility\n"
    for key in target_predict_dict:
        output = output + mol_name + "," + uni_pro[key] + "," + str(target_predict_dict[key]) + "\n"
    if not os.path.exists('./result_output'):
        os.mkdir('./result_output')
    with open('./result_output' + "/target_predict.txt","w") as rt:
        rt.write(output)

def pair_predict(drug_name,drug_smi,target_name,target_fas,tag):
    # drug_smi = dg_seq2num(drug_smi)
    # dg_input = []
    # dg_input.append(drug_smi)
    # dg_input.append(drug_smi)
    # dg_input = np.array(dg_input)
    # dg_input = torch.LongTensor(dg_input)
    fatoms = []
    fbonds = []
    agraph = []
    bgraph = []
    shape_tensor = []
    for i in range(2):
        [a, b, c, d, e] = smiles2mpnnfeature(drug_smi)
        fatoms.append(a)
        fbonds.append(b)
        agraph.append(c)
        bgraph.append(d)
        shape_tensor.append(e)

    v_d = []
    v_d.append(torch.stack(fatoms))
    v_d.append(torch.stack(fbonds))
    v_d.append(torch.stack(agraph))
    v_d.append(torch.stack(bgraph))
    v_d.append(torch.stack(shape_tensor).squeeze(1))
    dg_input = v_d

    target_fas = tg_seq2sum(target_fas)
    tg_input = []
    tg_input.append(target_fas)
    tg_input.append(target_fas)
    tg_input = np.array(tg_input)
    tg_input = torch.LongTensor(tg_input).to("cuda")

    with torch.no_grad():
        dg_common, tg_common= com_model(dg_input,tg_input)
        predict_rate,tag_p = pre_model(dg_common,tg_common,tag)
        # print(drug_name,target_name,tag,predict_rate[0].item())
    return drug_name, target_name, predict_rate[0].item(), tag

def final_process(num):
    out_put = "drug_name,uniprot,y_pre,y_label\n"
    out_set = data_process()
    for i in out_set:
        # print(i[0],i[2],i[1],i[3],i[4])
        drug_name, target_name, predic, label = pair_predict(i[0],i[2],i[1],i[3],i[4])
        out_put = out_put + drug_name + "," + target_name + "," + str(predic) + "," + str(label) + "\n"
    with open("./results/result_" + num + "/outtest_pre_lab.csv", "w") as f:
        f.write(out_put)

def main():
    num = str(sys.argv[1])
    global com_model, pre_model
    # out_set = data_process()
    com_parm, pre_parm = load_mod(num)
    com_model.load_state_dict(com_parm)
    pre_model.load_state_dict(pre_parm)
    com_model = com_model.to("cuda")
    pre_model = pre_model.to("cuda")
    com_model.eval()
    pre_model.eval()
    final_process(num)


if __name__=="__main__":
    main()
```

---

## ä¸€äº›bug

